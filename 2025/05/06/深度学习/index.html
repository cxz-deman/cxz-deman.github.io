<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="CXZ">





<title>深度学习 | CXZ_note</title>



    <link rel="icon" href="/head.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">CXZ&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">CXZ&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">深度学习</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">CXZ</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">May 6, 2025&nbsp;&nbsp;18:35:31</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="1认识torch"><a href="#1认识torch" class="headerlink" title="1认识torch"></a>1认识torch</h2><h3 id="1-1创建torch"><a href="#1-1创建torch" class="headerlink" title="1.1创建torch"></a>1.1创建torch</h3><h4 id="1-1-1检测能否使用cuda"><a href="#1-1-1检测能否使用cuda" class="headerlink" title="1.1.1检测能否使用cuda"></a>1.1.1检测能否使用cuda</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#检测能否使用cuda</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;cuda is available&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;cuda is not available&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="1-1-2创建torch"><a href="#1-1-2创建torch" class="headerlink" title="1.1.2创建torch"></a>1.1.2创建torch</h4><h5 id="1-1-2-1创建原始张量"><a href="#1-1-2-1创建原始张量" class="headerlink" title="1.1.2.1创建原始张量"></a>1.1.2.1创建原始张量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a=torch.tensor([1,2,3],dtype=torch.float32,device=&#x27;cuda&#x27;)</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line">print(a.size())#获取张量的形状</span><br><span class="line">print(a.shape)#获取张量的形状</span><br><span class="line">print(a.dtype)#获取张量的数据类型，如果在创建张量时没有指定dtype，则自动根据输入数组的拉数据类型判断</span><br></pre></td></tr></table></figure>

<p>获取张量的形状：</p>
<p>张量.size() &#x2F; .shape</p>
<p>获取张量的类型：</p>
<p>张量.dtype:torch.float32 、torch.float64</p>
<p>获取张量的设备：</p>
<p>张量.device 可以指定为’cuda’ 或者’cpu’</p>
<h5 id="1-1-2-2设置随机种子创建"><a href="#1-1-2-2设置随机种子创建" class="headerlink" title="1.1.2.2设置随机种子创建"></a>1.1.2.2设置随机种子创建</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置随机种子，目的是使每一次产生的随机数相同</span></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line">s=torch.randint(<span class="number">0</span>,<span class="number">100</span>,(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(s)</span><br></pre></td></tr></table></figure>

<h5 id="1-1-2-3创建符合正态分布的张量"><a href="#1-1-2-3创建符合正态分布的张量" class="headerlink" title="1.1.2.3创建符合正态分布的张量"></a>1.1.2.3创建符合正态分布的张量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#randn:符合标准正态分布的随机数，均值为0，标准差为1</span><br><span class="line">t1=torch.randn(1,10)</span><br><span class="line">print(t1)</span><br></pre></td></tr></table></figure>

<h5 id="1-1-2-4切换设备创建张量"><a href="#1-1-2-4切换设备创建张量" class="headerlink" title="1.1.2.4切换设备创建张量"></a>1.1.2.4<strong>切换设备创建张量</strong></h5><ul>
<li>除了第一种方法，其他所有都需要重新赋值</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">#创建张量时指定device</span><br><span class="line">t1=torch.tensor([1,2,3],device=&#x27;cuda&#x27;)</span><br><span class="line">print(t1)</span><br><span class="line"></span><br><span class="line">#使用to方法时运算时转换成其他设备</span><br><span class="line">t2=torch.tensor([1,2,3])</span><br><span class="line">t2=t2.to(&#x27;cuda&#x27;)</span><br><span class="line">print(t2)</span><br><span class="line"></span><br><span class="line">#使用cuda()或者cpu()方法切换</span><br><span class="line">t3=torch.tensor([1,2,3],device=&#x27;cuda&#x27;)</span><br><span class="line">t3=t3.cpu()</span><br><span class="line">print(t3,t3.device)</span><br><span class="line"></span><br><span class="line">t3=t3.cuda()</span><br><span class="line">print(t3,t3.device)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([1, 2, 3]) cpu</span><br><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;) cuda:0</span><br></pre></td></tr></table></figure>

<h3 id="1-2numpy和torch的转换"><a href="#1-2numpy和torch的转换" class="headerlink" title="1.2numpy和torch的转换"></a>1.2numpy和torch的转换</h3><p>涉及api</p>
<p>浅拷贝：张量.numpy()、torch.from_numpy(numpy数组)</p>
<p>深拷贝：张量.numpy().copy()、torch.tensor(numpy数组)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line">def test01():</span><br><span class="line">    #浅拷贝</span><br><span class="line">    t1=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">    n1=t1.numpy()</span><br><span class="line">    print(n1)</span><br><span class="line">    </span><br><span class="line">    #深拷贝</span><br><span class="line">    n2=t1.numpy().copy()</span><br><span class="line">    print(n2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    </span><br><span class="line">def test02():</span><br><span class="line">    #numpy转换成tensor</span><br><span class="line">    #浅拷贝: torch.from_numpy() 数据内存共享</span><br><span class="line">    #深拷贝：torch.tensor 创建新的副本</span><br><span class="line">    n1=np.array([[1,2,3],[4,5,6]])</span><br><span class="line">    t1=torch.from_numpy(n1)</span><br><span class="line">    print(t1)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    test01()</span><br><span class="line">    test02()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6]], dtype=torch.int32)</span><br></pre></td></tr></table></figure>

<h3 id="1-3提取单个元素的tensor"><a href="#1-3提取单个元素的tensor" class="headerlink" title="1.3提取单个元素的tensor"></a>1.3提取单个元素的tensor</h3><ul>
<li>item(),从单个元素的tensor中,无论几维都将转化为标量</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_element</span>():</span><br><span class="line">    data=torch.tensor([[<span class="number">90</span>]])</span><br><span class="line">    <span class="built_in">print</span>(data.item())</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    get_element()</span><br></pre></td></tr></table></figure>

<h3 id="1-4tensor的运算"><a href="#1-4tensor的运算" class="headerlink" title="1.4tensor的运算"></a>1.4tensor的运算</h3><ul>
<li>tensor运算函数：如果函数名后带有下划线则表示该方法为原地修改，否则表示不修改原始值，返回一个新对象</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def calculate():</span><br><span class="line">    data=torch.randint(1,10,(3,3))</span><br><span class="line">    print(data)</span><br><span class="line">    #不修改data</span><br><span class="line">    print(data.add(1))#运算所有元素+1</span><br><span class="line">    print(data.sub(1))#运算所有元素-1</span><br><span class="line">    print(data.mul(2))#运算所有元素*2</span><br><span class="line">    print(data.div(3))#运算所有元素/3</span><br><span class="line">    print(data.pow(2))#运算所有元素**2</span><br><span class="line">    </span><br><span class="line">    print(&quot;___________________&quot;)</span><br><span class="line">    #修改data</span><br><span class="line">    print(data.add_(1))#运算所有元素+1</span><br><span class="line">    print(data.sub_(1))#运算所有元素-1</span><br><span class="line">    print(data.mul_(2))#运算所有元素*2</span><br><span class="line">    print(data.pow_(2))#运算所有元素**2</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    calculate()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">tensor([[6, 1, 8],</span><br><span class="line">        [1, 5, 3],</span><br><span class="line">        [1, 5, 2]])</span><br><span class="line">tensor([[7, 2, 9],</span><br><span class="line">        [2, 6, 4],</span><br><span class="line">        [2, 6, 3]])</span><br><span class="line">tensor([[5, 0, 7],</span><br><span class="line">        [0, 4, 2],</span><br><span class="line">        [0, 4, 1]])</span><br><span class="line">tensor([[12,  2, 16],</span><br><span class="line">        [ 2, 10,  6],</span><br><span class="line">        [ 2, 10,  4]])</span><br><span class="line">tensor([[2.0000, 0.3333, 2.6667],</span><br><span class="line">        [0.3333, 1.6667, 1.0000],</span><br><span class="line">        [0.3333, 1.6667, 0.6667]])</span><br><span class="line">tensor([[36,  1, 64],</span><br><span class="line">        [ 1, 25,  9],</span><br><span class="line">        [ 1, 25,  4]])</span><br><span class="line">======================================</span><br><span class="line">tensor([[7, 2, 9],</span><br><span class="line">        [2, 6, 4],</span><br><span class="line">        [2, 6, 3]])</span><br><span class="line">tensor([[6, 1, 8],</span><br><span class="line">        [1, 5, 3],</span><br><span class="line">        [1, 5, 2]])</span><br><span class="line">tensor([[12,  2, 16],</span><br><span class="line">        [ 2, 10,  6],</span><br><span class="line">        [ 2, 10,  4]])</span><br><span class="line">tensor([[144,   4, 256],</span><br><span class="line">        [  4, 100,  36],</span><br><span class="line">        [  4, 100,  16]])</span><br></pre></td></tr></table></figure>

<h3 id="1-5阿达玛积"><a href="#1-5阿达玛积" class="headerlink" title="1.5阿达玛积"></a>1.5阿达玛积</h3><p>阿达玛积与矩阵相乘的区别</p>
<p>阿达玛积：两个相同的矩阵对应的位置元素相乘，得出的新矩阵得出的矩阵</p>
<p>矩阵相乘：(m,p)和(p,n)形状的矩阵相乘，结果为(m,n)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def adama_mul():</span><br><span class="line">    data1=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">    data2=torch.tensor([[2,3,4],[5,6,7]])</span><br><span class="line">    print(data1*data2)#对应相乘</span><br><span class="line"></span><br><span class="line">def adama_mul_1():</span><br><span class="line">    data1=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">    data2=torch.tensor([[2,3,4],[5,6,7]])</span><br><span class="line">    print(data1.mul(data2))#对应相乘</span><br><span class="line">    </span><br><span class="line">def 矩阵相乘():</span><br><span class="line">    data1=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">    data2=torch.tensor([[2,3,4],[5,6,7]])</span><br><span class="line">    print(data1@data2.T)</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    adama_mul()</span><br><span class="line">    adama_mul_1()</span><br><span class="line">    矩阵相乘()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 2,  6, 12],</span><br><span class="line">        [20, 30, 42]])</span><br><span class="line">tensor([[ 2,  6, 12],</span><br><span class="line">        [20, 30, 42]])</span><br><span class="line">tensor([[20, 38],</span><br><span class="line">        [47, 92]])</span><br></pre></td></tr></table></figure>

<h3 id="1-6改变形状"><a href="#1-6改变形状" class="headerlink" title="1.6改变形状"></a>1.6改变形状</h3><h4 id="1-6-1一般改变形状"><a href="#1-6-1一般改变形状" class="headerlink" title="1.6.1一般改变形状"></a>1.6.1一般改变形状</h4><p>view:与reshape方法类似，和reshape区别，view在内存中连续，按c顺序存储</p>
<p>如果是数据不连续：使用view方法会报错，可以使用reshape代替</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">def change_shape():</span><br><span class="line">    data=torch.randint(0,100,(2,3,3))</span><br><span class="line">    print(data.shape)</span><br><span class="line">    print(data)</span><br><span class="line">    print(data.is_contiguous())#判断是否连续</span><br><span class="line">    print(data.reshape(3,-1))</span><br><span class="line">    print(data.view(3,-1))#需要判断数据是否连续，利用is_contiguous()</span><br><span class="line">    </span><br><span class="line">    t3=t1.t()#转置</span><br><span class="line">    print(t3.is_contiguous())</span><br><span class="line">    </span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    change_shape()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 3])</span><br><span class="line">tensor([[[29, 56, 89],</span><br><span class="line">         [54, 70, 91],</span><br><span class="line">         [30, 76, 78]],</span><br><span class="line"></span><br><span class="line">        [[93, 46, 27],</span><br><span class="line">         [33, 28, 67],</span><br><span class="line">         [98, 90, 41]]])</span><br><span class="line">True</span><br><span class="line">tensor([[29, 56, 89, 54, 70, 91],</span><br><span class="line">        [30, 76, 78, 93, 46, 27],</span><br><span class="line">        [33, 28, 67, 98, 90, 41]])</span><br><span class="line">tensor([[29, 56, 89, 54, 70, 91],</span><br><span class="line">        [30, 76, 78, 93, 46, 27],</span><br><span class="line">        [33, 28, 67, 98, 90, 41]])</span><br><span class="line">True</span><br></pre></td></tr></table></figure>

<h4 id="1-6-2交换张量维度改变形状"><a href="#1-6-2交换张量维度改变形状" class="headerlink" title="1.6.2交换张量维度改变形状"></a>1.6.2交换张量维度改变形状</h4><p>transpose()：只能两两交换，用于交换张量的两个维度，一次只能交换两个</p>
<p>permute():  用于交换张量的多个维度来改变形状</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def change_ndim():</span><br><span class="line">    t1=torch.randint(1,10,(2,3,4))</span><br><span class="line">    print(t1.transpose(0,1))</span><br><span class="line">    print(t1.transpose(0,1).shape)</span><br><span class="line">    print(t1.permute(1,2,0))</span><br><span class="line">    print(t1.permute(1,2,0).shape)</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    change_ndim()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[3, 6, 2, 1],</span><br><span class="line">         [4, 1, 3, 1]],</span><br><span class="line"></span><br><span class="line">        [[5, 3, 6, 7],</span><br><span class="line">         [6, 1, 3, 1]],</span><br><span class="line"></span><br><span class="line">        [[1, 4, 4, 4],</span><br><span class="line">         [8, 1, 6, 4]]])</span><br><span class="line">torch.Size([3, 2, 4])</span><br><span class="line">tensor([[[3, 4],</span><br><span class="line">         [6, 1],</span><br><span class="line">         [2, 3],</span><br><span class="line">         [1, 1]],</span><br><span class="line"></span><br><span class="line">        [[5, 6],</span><br><span class="line">         [3, 1],</span><br><span class="line">         [6, 3],</span><br><span class="line">         [7, 1]],</span><br><span class="line"></span><br><span class="line">        [[1, 8],</span><br><span class="line">         [4, 1],</span><br><span class="line">         [4, 6],</span><br><span class="line">         [4, 4]]])</span><br><span class="line">torch.Size([3, 4, 2])</span><br></pre></td></tr></table></figure>

<h3 id="1-7升维与降维"><a href="#1-7升维与降维" class="headerlink" title="1.7升维与降维"></a>1.7升维与降维</h3><p>unsqueeze(n)：升维，在第n个位置添加维度数为1的维度</p>
<p>squeeze(n)：降维，删除第n个位置添加维度数为1的维度，但是如果指定的维度不为1，则不做操作也不报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">def un():</span><br><span class="line">    t1=torch.randint(1,10,(2,3,4))</span><br><span class="line">    #升维</span><br><span class="line">    t2=t1.unsqueeze(2)</span><br><span class="line">    print(t2.shape)</span><br><span class="line">    print(t2)</span><br><span class="line">    #降维</span><br><span class="line">    #默认删除所有维度数为1的数，如果指定的维度的维度数不为1则不做任何操作，也不报错</span><br><span class="line">    t3=t2.squeeze()</span><br><span class="line">    print(t3.shape)</span><br><span class="line">    print(t3)</span><br><span class="line">    #此处不做操作</span><br><span class="line">    t4=t2.squeeze(3)</span><br><span class="line">    print(t4.shape)</span><br><span class="line">    print(t4)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    un()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 1, 4])</span><br><span class="line">tensor([[[[1, 1, 6, 8]],</span><br><span class="line"></span><br><span class="line">         [[7, 4, 3, 3]],</span><br><span class="line"></span><br><span class="line">         [[7, 2, 9, 8]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[8, 1, 3, 5]],</span><br><span class="line"></span><br><span class="line">         [[8, 7, 7, 3]],</span><br><span class="line"></span><br><span class="line">         [[7, 2, 5, 6]]]])</span><br><span class="line">torch.Size([2, 3, 4])</span><br><span class="line">tensor([[[1, 1, 6, 8],</span><br><span class="line">         [7, 4, 3, 3],</span><br><span class="line">         [7, 2, 9, 8]],</span><br><span class="line"></span><br><span class="line">        [[8, 1, 3, 5],</span><br><span class="line">         [8, 7, 7, 3],</span><br><span class="line">         [7, 2, 5, 6]]])</span><br><span class="line">torch.Size([2, 3, 1, 4])</span><br><span class="line">tensor([[[[1, 1, 6, 8]],</span><br><span class="line"></span><br><span class="line">         [[7, 4, 3, 3]],</span><br><span class="line"></span><br><span class="line">         [[7, 2, 9, 8]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[8, 1, 3, 5]],</span><br><span class="line"></span><br><span class="line">         [[8, 7, 7, 3]],</span><br><span class="line"></span><br><span class="line">         [[7, 2, 5, 6]]]])</span><br></pre></td></tr></table></figure>

<h3 id="1-8广播机制"><a href="#1-8广播机制" class="headerlink" title="1.8广播机制"></a>1.8广播机制</h3><p>torch的广播机制</p>
<p>numpy一样，torch也支持广播机制。</p>
<p>与numpy类似不作过多赘述</p>
<h3 id="1-9自动求导"><a href="#1-9自动求导" class="headerlink" title="1.9自动求导"></a>1.9自动求导</h3><h4 id="1-9-1一元求导"><a href="#1-9-1一元求导" class="headerlink" title="1.9.1一元求导"></a>1.9.1一元求导</h4><p>1.叶子节点张量要添加requires_grad&#x3D;True</p>
<p>2.叶子节点的数据类型要是浮点数</p>
<p>3.调用backward()，可以自动求导</p>
<p>4.求导结果保存在x(叶子结点  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_func</span>():</span><br><span class="line">    x=torch.tensor(<span class="number">1</span>,requires_grad=<span class="literal">True</span>,dtype=torch.float32)</span><br><span class="line">    y = x ** <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 求导,反向传播作自动求导</span></span><br><span class="line">    y.backward()</span><br><span class="line">    <span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>

<p>当x的维度不为1时</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def grad_xiang():</span><br><span class="line">    x=torch.tensor([[1,2,3],[4,5,6]],requires_grad=True,dtype=torch.float32)</span><br><span class="line">    y = x ** 2</span><br><span class="line">    #1、指定初始值y</span><br><span class="line">    # y.backward(torch.tensor([[1.0,1.0,1.0],[1.0,1.0,1.0]]))</span><br><span class="line">    # print(x.grad)</span><br><span class="line">    </span><br><span class="line">    #2、把梯度向量通过类似计算损失的方式将输出转换为标量，然后再调用backward()</span><br><span class="line">    z=y.sum()</span><br><span class="line">    z.backward()</span><br><span class="line">    print(x.grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    grad_func()</span><br><span class="line">    grad_xiang()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(2.)</span><br><span class="line">tensor([[ 2.,  4.,  6.],</span><br><span class="line">        [ 8., 10., 12.]])</span><br></pre></td></tr></table></figure>

<h4 id="1-9-2二元求导"><a href="#1-9-2二元求导" class="headerlink" title="1.9.2二元求导"></a>1.9.2二元求导</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def grad_X_Y():</span><br><span class="line">    x=torch.tensor([1,2,3],requires_grad=True,dtype=torch.float)</span><br><span class="line">    y=torch.tensor([4,5,6],requires_grad=True,dtype=torch.float)</span><br><span class="line">    z=x*y</span><br><span class="line">    loss=z.sum()</span><br><span class="line">    loss.backward()</span><br><span class="line">    print(x.grad,y.grad)</span><br><span class="line">    print(z.grad)#z是中间变量，在反向传播中也会参与梯度甲酸，但是在计算完成后将梯度清除</span><br><span class="line">    grad_X_Y()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([4., 5., 6.]) tensor([1., 2., 3.])</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<p>在z释放后故输出None</p>
<h3 id="1-10控制变量不进行梯度计算"><a href="#1-10控制变量不进行梯度计算" class="headerlink" title="1.10控制变量不进行梯度计算"></a>1.10控制变量不进行梯度计算</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def control_grad():</span><br><span class="line">    x=torch.tensor(1.0,requires_grad=True,dtype=torch.float32)</span><br><span class="line">    </span><br><span class="line">    with torch.no_grad():#禁止该上下文的代码参与梯度计算</span><br><span class="line">        y=x**2+2*x+3</span><br><span class="line">    print(y.requires_grad)</span><br><span class="line">def control_grad2():</span><br><span class="line">    x=torch.tensor([1,2,3],requires_grad=True,dtype=torch.float32)</span><br><span class="line">    # x和y的映射函数要放在循环内部</span><br><span class="line">    # 计算图中叶子节点梯度默认是累加的，所以要清零</span><br><span class="line">    # 不希望叶子节点梯度累加，累加没办法进行梯度更新，需要对每轮次的梯度进行清零</span><br><span class="line">    for epoch in range(3):</span><br><span class="line">        if x.grad is not None: # 梯度清零</span><br><span class="line">            x.grad.zero_()</span><br><span class="line">        y=x**2</span><br><span class="line">        z=y.sum()</span><br><span class="line">        z.backward()</span><br><span class="line">        print(x.grad)</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    control_grad()</span><br><span class="line">    control_grad2()</span><br></pre></td></tr></table></figure>

<p>x和y的映射函数要放在循环内部</p>
<p> 计算图中叶子节点梯度默认是累加的，所以要清零</p>
<p> 不希望叶子节点梯度累加，累加没办法进行梯度更新，需要对每轮次的梯度进行清零</p>
<p>不进行清零就会输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">False</span><br><span class="line">tensor([2., 4., 6.])</span><br><span class="line">tensor([4., 8., 12.])</span><br><span class="line">tensor([6., 12., 18.])</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">False</span><br><span class="line">tensor([2., 4., 6.])</span><br><span class="line">tensor([2., 4., 6.])</span><br><span class="line">tensor([2., 4., 6.])</span><br></pre></td></tr></table></figure>


        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>CXZ</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://cxz-deman.github.io/2025/05/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">https://cxz-deman.github.io/2025/05/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>god</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%90%88%E9%9B%86/"># 阶段学习笔记合集</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2025/05/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">机器学习</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© CXZ | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>