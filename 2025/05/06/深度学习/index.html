<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="CXZ">





<title>深度学习 | CXZ_note</title>



    <link rel="icon" href="/head.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">CXZ&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">CXZ&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">深度学习</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">CXZ</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">May 6, 2025&nbsp;&nbsp;18:35:31</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="1认识torch"><a href="#1认识torch" class="headerlink" title="1认识torch"></a>1认识torch</h2><h3 id="1-1创建torch"><a href="#1-1创建torch" class="headerlink" title="1.1创建torch"></a>1.1创建torch</h3><h4 id="1-1-1检测能否使用cuda"><a href="#1-1-1检测能否使用cuda" class="headerlink" title="1.1.1检测能否使用cuda"></a>1.1.1检测能否使用cuda</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#检测能否使用cuda</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;cuda is available&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;cuda is not available&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="1-1-2创建torch"><a href="#1-1-2创建torch" class="headerlink" title="1.1.2创建torch"></a>1.1.2创建torch</h4><h5 id="1-1-2-1创建原始张量"><a href="#1-1-2-1创建原始张量" class="headerlink" title="1.1.2.1创建原始张量"></a>1.1.2.1创建原始张量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a=torch.tensor([1,2,3],dtype=torch.float32,device=&#x27;cuda&#x27;)</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line">print(a.size())#获取张量的形状</span><br><span class="line">print(a.shape)#获取张量的形状</span><br><span class="line">print(a.dtype)#获取张量的数据类型，如果在创建张量时没有指定dtype，则自动根据输入数组的拉数据类型判断</span><br></pre></td></tr></table></figure>

<p>获取张量的形状：</p>
<p>张量.size() &#x2F; .shape</p>
<p>获取张量的类型：</p>
<p>张量.dtype:torch.float32 、torch.float64</p>
<p>获取张量的设备：</p>
<p>张量.device 可以指定为’cuda’ 或者’cpu’</p>
<h5 id="1-1-2-2设置随机种子创建"><a href="#1-1-2-2设置随机种子创建" class="headerlink" title="1.1.2.2设置随机种子创建"></a>1.1.2.2设置随机种子创建</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置随机种子，目的是使每一次产生的随机数相同</span></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line">s=torch.randint(<span class="number">0</span>,<span class="number">100</span>,(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(s)</span><br></pre></td></tr></table></figure>

<h5 id="1-1-2-3创建符合正态分布的张量"><a href="#1-1-2-3创建符合正态分布的张量" class="headerlink" title="1.1.2.3创建符合正态分布的张量"></a>1.1.2.3创建符合正态分布的张量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#randn:符合标准正态分布的随机数，均值为0，标准差为1</span><br><span class="line">t1=torch.randn(1,10)</span><br><span class="line">print(t1)</span><br></pre></td></tr></table></figure>

<h5 id="1-1-2-4切换设备创建张量"><a href="#1-1-2-4切换设备创建张量" class="headerlink" title="1.1.2.4切换设备创建张量"></a>1.1.2.4<strong>切换设备创建张量</strong></h5><ul>
<li>除了第一种方法，其他所有都需要重新赋值</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">#创建张量时指定device</span><br><span class="line">t1=torch.tensor([1,2,3],device=&#x27;cuda&#x27;)</span><br><span class="line">print(t1)</span><br><span class="line"></span><br><span class="line">#使用to方法时运算时转换成其他设备</span><br><span class="line">t2=torch.tensor([1,2,3])</span><br><span class="line">t2=t2.to(&#x27;cuda&#x27;)</span><br><span class="line">print(t2)</span><br><span class="line"></span><br><span class="line">#使用cuda()或者cpu()方法切换</span><br><span class="line">t3=torch.tensor([1,2,3],device=&#x27;cuda&#x27;)</span><br><span class="line">t3=t3.cpu()</span><br><span class="line">print(t3,t3.device)</span><br><span class="line"></span><br><span class="line">t3=t3.cuda()</span><br><span class="line">print(t3,t3.device)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([1, 2, 3]) cpu</span><br><span class="line">tensor([1, 2, 3], device=&#x27;cuda:0&#x27;) cuda:0</span><br></pre></td></tr></table></figure>

<h3 id="1-2numpy和torch的转换"><a href="#1-2numpy和torch的转换" class="headerlink" title="1.2numpy和torch的转换"></a>1.2numpy和torch的转换</h3><p>涉及api</p>
<p>浅拷贝：张量.numpy()、torch.from_numpy(numpy数组)</p>
<p>深拷贝：张量.numpy().copy()、torch.tensor(numpy数组)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line">def test01():</span><br><span class="line">    #浅拷贝</span><br><span class="line">    t1=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">    n1=t1.numpy()</span><br><span class="line">    print(n1)</span><br><span class="line">    </span><br><span class="line">    #深拷贝</span><br><span class="line">    n2=t1.numpy().copy()</span><br><span class="line">    print(n2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    </span><br><span class="line">def test02():</span><br><span class="line">    #numpy转换成tensor</span><br><span class="line">    #浅拷贝: torch.from_numpy() 数据内存共享</span><br><span class="line">    #深拷贝：torch.tensor 创建新的副本</span><br><span class="line">    n1=np.array([[1,2,3],[4,5,6]])</span><br><span class="line">    t1=torch.from_numpy(n1)</span><br><span class="line">    print(t1)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    test01()</span><br><span class="line">    test02()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6]], dtype=torch.int32)</span><br></pre></td></tr></table></figure>

<h3 id="1-3提取单个元素的tensor"><a href="#1-3提取单个元素的tensor" class="headerlink" title="1.3提取单个元素的tensor"></a>1.3提取单个元素的tensor</h3><ul>
<li>item(),从单个元素的tensor中,无论几维都将转化为标量</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_element</span>():</span><br><span class="line">    data=torch.tensor([[<span class="number">90</span>]])</span><br><span class="line">    <span class="built_in">print</span>(data.item())</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    get_element()</span><br></pre></td></tr></table></figure>

<h3 id="1-4tensor的运算"><a href="#1-4tensor的运算" class="headerlink" title="1.4tensor的运算"></a>1.4tensor的运算</h3><ul>
<li>tensor运算函数：如果函数名后带有下划线则表示该方法为原地修改，否则表示不修改原始值，返回一个新对象</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def calculate():</span><br><span class="line">    data=torch.randint(1,10,(3,3))</span><br><span class="line">    print(data)</span><br><span class="line">    #不修改data</span><br><span class="line">    print(data.add(1))#运算所有元素+1</span><br><span class="line">    print(data.sub(1))#运算所有元素-1</span><br><span class="line">    print(data.mul(2))#运算所有元素*2</span><br><span class="line">    print(data.div(3))#运算所有元素/3</span><br><span class="line">    print(data.pow(2))#运算所有元素**2</span><br><span class="line">    </span><br><span class="line">    print(&quot;___________________&quot;)</span><br><span class="line">    #修改data</span><br><span class="line">    print(data.add_(1))#运算所有元素+1</span><br><span class="line">    print(data.sub_(1))#运算所有元素-1</span><br><span class="line">    print(data.mul_(2))#运算所有元素*2</span><br><span class="line">    print(data.pow_(2))#运算所有元素**2</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    calculate()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">tensor([[6, 1, 8],</span><br><span class="line">        [1, 5, 3],</span><br><span class="line">        [1, 5, 2]])</span><br><span class="line">tensor([[7, 2, 9],</span><br><span class="line">        [2, 6, 4],</span><br><span class="line">        [2, 6, 3]])</span><br><span class="line">tensor([[5, 0, 7],</span><br><span class="line">        [0, 4, 2],</span><br><span class="line">        [0, 4, 1]])</span><br><span class="line">tensor([[12,  2, 16],</span><br><span class="line">        [ 2, 10,  6],</span><br><span class="line">        [ 2, 10,  4]])</span><br><span class="line">tensor([[2.0000, 0.3333, 2.6667],</span><br><span class="line">        [0.3333, 1.6667, 1.0000],</span><br><span class="line">        [0.3333, 1.6667, 0.6667]])</span><br><span class="line">tensor([[36,  1, 64],</span><br><span class="line">        [ 1, 25,  9],</span><br><span class="line">        [ 1, 25,  4]])</span><br><span class="line">======================================</span><br><span class="line">tensor([[7, 2, 9],</span><br><span class="line">        [2, 6, 4],</span><br><span class="line">        [2, 6, 3]])</span><br><span class="line">tensor([[6, 1, 8],</span><br><span class="line">        [1, 5, 3],</span><br><span class="line">        [1, 5, 2]])</span><br><span class="line">tensor([[12,  2, 16],</span><br><span class="line">        [ 2, 10,  6],</span><br><span class="line">        [ 2, 10,  4]])</span><br><span class="line">tensor([[144,   4, 256],</span><br><span class="line">        [  4, 100,  36],</span><br><span class="line">        [  4, 100,  16]])</span><br></pre></td></tr></table></figure>

<h3 id="1-5阿达玛积"><a href="#1-5阿达玛积" class="headerlink" title="1.5阿达玛积"></a>1.5阿达玛积</h3><p>阿达玛积与矩阵相乘的区别</p>
<p>阿达玛积：两个相同的矩阵对应的位置元素相乘，得出的新矩阵得出的矩阵</p>
<p>矩阵相乘：(m,p)和(p,n)形状的矩阵相乘，结果为(m,n)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def adama_mul():</span><br><span class="line">    data1=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">    data2=torch.tensor([[2,3,4],[5,6,7]])</span><br><span class="line">    print(data1*data2)#对应相乘</span><br><span class="line"></span><br><span class="line">def adama_mul_1():</span><br><span class="line">    data1=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">    data2=torch.tensor([[2,3,4],[5,6,7]])</span><br><span class="line">    print(data1.mul(data2))#对应相乘</span><br><span class="line">    </span><br><span class="line">def 矩阵相乘():</span><br><span class="line">    data1=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">    data2=torch.tensor([[2,3,4],[5,6,7]])</span><br><span class="line">    print(data1@data2.T)</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    adama_mul()</span><br><span class="line">    adama_mul_1()</span><br><span class="line">    矩阵相乘()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 2,  6, 12],</span><br><span class="line">        [20, 30, 42]])</span><br><span class="line">tensor([[ 2,  6, 12],</span><br><span class="line">        [20, 30, 42]])</span><br><span class="line">tensor([[20, 38],</span><br><span class="line">        [47, 92]])</span><br></pre></td></tr></table></figure>

<h3 id="1-6改变形状"><a href="#1-6改变形状" class="headerlink" title="1.6改变形状"></a>1.6改变形状</h3><h4 id="1-6-1一般改变形状"><a href="#1-6-1一般改变形状" class="headerlink" title="1.6.1一般改变形状"></a>1.6.1一般改变形状</h4><p>view:与reshape方法类似，和reshape区别，view在内存中连续，按c顺序存储</p>
<p>如果是数据不连续：使用view方法会报错，可以使用reshape代替</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">def change_shape():</span><br><span class="line">    data=torch.randint(0,100,(2,3,3))</span><br><span class="line">    print(data.shape)</span><br><span class="line">    print(data)</span><br><span class="line">    print(data.is_contiguous())#判断是否连续</span><br><span class="line">    print(data.reshape(3,-1))</span><br><span class="line">    print(data.view(3,-1))#需要判断数据是否连续，利用is_contiguous()</span><br><span class="line">    </span><br><span class="line">    t3=t1.t()#转置</span><br><span class="line">    print(t3.is_contiguous())</span><br><span class="line">    </span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    change_shape()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 3])</span><br><span class="line">tensor([[[29, 56, 89],</span><br><span class="line">         [54, 70, 91],</span><br><span class="line">         [30, 76, 78]],</span><br><span class="line"></span><br><span class="line">        [[93, 46, 27],</span><br><span class="line">         [33, 28, 67],</span><br><span class="line">         [98, 90, 41]]])</span><br><span class="line">True</span><br><span class="line">tensor([[29, 56, 89, 54, 70, 91],</span><br><span class="line">        [30, 76, 78, 93, 46, 27],</span><br><span class="line">        [33, 28, 67, 98, 90, 41]])</span><br><span class="line">tensor([[29, 56, 89, 54, 70, 91],</span><br><span class="line">        [30, 76, 78, 93, 46, 27],</span><br><span class="line">        [33, 28, 67, 98, 90, 41]])</span><br><span class="line">True</span><br></pre></td></tr></table></figure>

<h4 id="1-6-2交换张量维度改变形状"><a href="#1-6-2交换张量维度改变形状" class="headerlink" title="1.6.2交换张量维度改变形状"></a>1.6.2交换张量维度改变形状</h4><p>transpose()：只能两两交换，用于交换张量的两个维度，一次只能交换两个</p>
<p>permute():  用于交换张量的多个维度来改变形状</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def change_ndim():</span><br><span class="line">    t1=torch.randint(1,10,(2,3,4))</span><br><span class="line">    print(t1.transpose(0,1))</span><br><span class="line">    print(t1.transpose(0,1).shape)</span><br><span class="line">    print(t1.permute(1,2,0))</span><br><span class="line">    print(t1.permute(1,2,0).shape)</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    change_ndim()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[3, 6, 2, 1],</span><br><span class="line">         [4, 1, 3, 1]],</span><br><span class="line"></span><br><span class="line">        [[5, 3, 6, 7],</span><br><span class="line">         [6, 1, 3, 1]],</span><br><span class="line"></span><br><span class="line">        [[1, 4, 4, 4],</span><br><span class="line">         [8, 1, 6, 4]]])</span><br><span class="line">torch.Size([3, 2, 4])</span><br><span class="line">tensor([[[3, 4],</span><br><span class="line">         [6, 1],</span><br><span class="line">         [2, 3],</span><br><span class="line">         [1, 1]],</span><br><span class="line"></span><br><span class="line">        [[5, 6],</span><br><span class="line">         [3, 1],</span><br><span class="line">         [6, 3],</span><br><span class="line">         [7, 1]],</span><br><span class="line"></span><br><span class="line">        [[1, 8],</span><br><span class="line">         [4, 1],</span><br><span class="line">         [4, 6],</span><br><span class="line">         [4, 4]]])</span><br><span class="line">torch.Size([3, 4, 2])</span><br></pre></td></tr></table></figure>

<h3 id="1-7升维与降维"><a href="#1-7升维与降维" class="headerlink" title="1.7升维与降维"></a>1.7升维与降维</h3><p>unsqueeze(n)：升维，在第n个位置添加维度数为1的维度</p>
<p>squeeze(n)：降维，删除第n个位置添加维度数为1的维度，但是如果指定的维度不为1，则不做操作也不报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">def un():</span><br><span class="line">    t1=torch.randint(1,10,(2,3,4))</span><br><span class="line">    #升维</span><br><span class="line">    t2=t1.unsqueeze(2)</span><br><span class="line">    print(t2.shape)</span><br><span class="line">    print(t2)</span><br><span class="line">    #降维</span><br><span class="line">    #默认删除所有维度数为1的数，如果指定的维度的维度数不为1则不做任何操作，也不报错</span><br><span class="line">    t3=t2.squeeze()</span><br><span class="line">    print(t3.shape)</span><br><span class="line">    print(t3)</span><br><span class="line">    #此处不做操作</span><br><span class="line">    t4=t2.squeeze(3)</span><br><span class="line">    print(t4.shape)</span><br><span class="line">    print(t4)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    un()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 1, 4])</span><br><span class="line">tensor([[[[1, 1, 6, 8]],</span><br><span class="line"></span><br><span class="line">         [[7, 4, 3, 3]],</span><br><span class="line"></span><br><span class="line">         [[7, 2, 9, 8]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[8, 1, 3, 5]],</span><br><span class="line"></span><br><span class="line">         [[8, 7, 7, 3]],</span><br><span class="line"></span><br><span class="line">         [[7, 2, 5, 6]]]])</span><br><span class="line">torch.Size([2, 3, 4])</span><br><span class="line">tensor([[[1, 1, 6, 8],</span><br><span class="line">         [7, 4, 3, 3],</span><br><span class="line">         [7, 2, 9, 8]],</span><br><span class="line"></span><br><span class="line">        [[8, 1, 3, 5],</span><br><span class="line">         [8, 7, 7, 3],</span><br><span class="line">         [7, 2, 5, 6]]])</span><br><span class="line">torch.Size([2, 3, 1, 4])</span><br><span class="line">tensor([[[[1, 1, 6, 8]],</span><br><span class="line"></span><br><span class="line">         [[7, 4, 3, 3]],</span><br><span class="line"></span><br><span class="line">         [[7, 2, 9, 8]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[8, 1, 3, 5]],</span><br><span class="line"></span><br><span class="line">         [[8, 7, 7, 3]],</span><br><span class="line"></span><br><span class="line">         [[7, 2, 5, 6]]]])</span><br></pre></td></tr></table></figure>

<h3 id="1-8广播机制"><a href="#1-8广播机制" class="headerlink" title="1.8广播机制"></a>1.8广播机制</h3><p>torch的广播机制</p>
<p>numpy一样，torch也支持广播机制。</p>
<p>与numpy类似不作过多赘述</p>
<h3 id="1-9自动求导"><a href="#1-9自动求导" class="headerlink" title="1.9自动求导"></a>1.9自动求导</h3><h4 id="1-9-1一元求导"><a href="#1-9-1一元求导" class="headerlink" title="1.9.1一元求导"></a>1.9.1一元求导</h4><p>1.叶子节点张量要添加requires_grad&#x3D;True</p>
<p>2.叶子节点的数据类型要是浮点数</p>
<p>3.调用backward()，可以自动求导</p>
<p>4.求导结果保存在x(叶子结点  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_func</span>():</span><br><span class="line">    x=torch.tensor(<span class="number">1</span>,requires_grad=<span class="literal">True</span>,dtype=torch.float32)</span><br><span class="line">    y = x ** <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 求导,反向传播作自动求导</span></span><br><span class="line">    y.backward()</span><br><span class="line">    <span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>

<p>当x的维度不为1时</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def grad_xiang():</span><br><span class="line">    x=torch.tensor([[1,2,3],[4,5,6]],requires_grad=True,dtype=torch.float32)</span><br><span class="line">    y = x ** 2</span><br><span class="line">    #1、指定初始值y</span><br><span class="line">    # y.backward(torch.tensor([[1.0,1.0,1.0],[1.0,1.0,1.0]]))</span><br><span class="line">    # print(x.grad)</span><br><span class="line">    </span><br><span class="line">    #2、把梯度向量通过类似计算损失的方式将输出转换为标量，然后再调用backward()</span><br><span class="line">    z=y.sum()</span><br><span class="line">    z.backward()</span><br><span class="line">    print(x.grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    grad_func()</span><br><span class="line">    grad_xiang()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(2.)</span><br><span class="line">tensor([[ 2.,  4.,  6.],</span><br><span class="line">        [ 8., 10., 12.]])</span><br></pre></td></tr></table></figure>

<h4 id="1-9-2二元求导"><a href="#1-9-2二元求导" class="headerlink" title="1.9.2二元求导"></a>1.9.2二元求导</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def grad_X_Y():</span><br><span class="line">    x=torch.tensor([1,2,3],requires_grad=True,dtype=torch.float)</span><br><span class="line">    y=torch.tensor([4,5,6],requires_grad=True,dtype=torch.float)</span><br><span class="line">    z=x*y</span><br><span class="line">    loss=z.sum()</span><br><span class="line">    loss.backward()</span><br><span class="line">    print(x.grad,y.grad)</span><br><span class="line">    print(z.grad)#z是中间变量，在反向传播中也会参与梯度甲酸，但是在计算完成后将梯度清除</span><br><span class="line">    grad_X_Y()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([4., 5., 6.]) tensor([1., 2., 3.])</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<p>在z释放后故输出None</p>
<h3 id="1-10控制变量不进行梯度计算"><a href="#1-10控制变量不进行梯度计算" class="headerlink" title="1.10控制变量不进行梯度计算"></a>1.10控制变量不进行梯度计算</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def control_grad():</span><br><span class="line">    x=torch.tensor(1.0,requires_grad=True,dtype=torch.float32)</span><br><span class="line">    </span><br><span class="line">    with torch.no_grad():#禁止该上下文的代码参与梯度计算</span><br><span class="line">        y=x**2+2*x+3</span><br><span class="line">    print(y.requires_grad)</span><br><span class="line">def control_grad2():</span><br><span class="line">    x=torch.tensor([1,2,3],requires_grad=True,dtype=torch.float32)</span><br><span class="line">    # x和y的映射函数要放在循环内部</span><br><span class="line">    # 计算图中叶子节点梯度默认是累加的，所以要清零</span><br><span class="line">    # 不希望叶子节点梯度累加，累加没办法进行梯度更新，需要对每轮次的梯度进行清零</span><br><span class="line">    for epoch in range(3):</span><br><span class="line">        if x.grad is not None: # 梯度清零</span><br><span class="line">            x.grad.zero_()</span><br><span class="line">        y=x**2</span><br><span class="line">        z=y.sum()</span><br><span class="line">        z.backward()</span><br><span class="line">        print(x.grad)</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    control_grad()</span><br><span class="line">    control_grad2()</span><br></pre></td></tr></table></figure>

<p>x和y的映射函数要放在循环内部</p>
<p> 计算图中叶子节点梯度默认是累加的，所以要清零</p>
<p> 不希望叶子节点梯度累加，累加没办法进行梯度更新，需要对每轮次的梯度进行清零</p>
<p>不进行清零就会输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">False</span><br><span class="line">tensor([2., 4., 6.])</span><br><span class="line">tensor([4., 8., 12.])</span><br><span class="line">tensor([6., 12., 18.])</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">False</span><br><span class="line">tensor([2., 4., 6.])</span><br><span class="line">tensor([2., 4., 6.])</span><br><span class="line">tensor([2., 4., 6.])</span><br></pre></td></tr></table></figure>

<h2 id="2创建神经网络层"><a href="#2创建神经网络层" class="headerlink" title="2创建神经网络层"></a>2创建神经网络层</h2><h3 id="2-1搭建层"><a href="#2-1搭建层" class="headerlink" title="2.1搭建层"></a>2.1搭建层</h3><h4 id="2-1-1使用类来创建"><a href="#2-1-1使用类来创建" class="headerlink" title="2.1.1使用类来创建"></a>2.1.1使用类来创建</h4><p>常用api:</p>
<p>nn.Linear(数据的特征数，int)</p>
<p>nn.Linear(int,int)</p>
<p>nn.Linear(int,输出的标签数)</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyFcnn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,input_size,out_size</span>):</span><br><span class="line">        <span class="comment">#父类初始化</span></span><br><span class="line">        <span class="built_in">super</span>(MyFcnn,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment">#定义线性层1</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(input_size,<span class="number">64</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#定义线性层2</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#定义线性层3</span></span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">32</span>,out_size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#定义前向传播</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">input_size = <span class="number">784</span></span><br><span class="line">out_size = <span class="number">1</span></span><br><span class="line">model = MyFcnn(input_size,out_size)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MyFcnn(</span><br><span class="line">  (fc1): Linear(in_features=784, out_features=64, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=64, out_features=32, bias=True)</span><br><span class="line">  (fc3): Linear(in_features=32, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>步骤：#1、构建类需要继承一个抽象类，nn.Module</p>
<p>​			#2、定义__init__()方法，定义网络结构</p>
<p>​			#3、实现forward()方法，定义前向传播的顺序</p>
<h4 id="2-1-2利用Sequential创建"><a href="#2-1-2利用Sequential创建" class="headerlink" title="2.1.2利用Sequential创建"></a>2.1.2利用Sequential创建</h4><p>#nn.Sequential快速创建神经网络的方法</p>
<p>#可以自动实现forward方法</p>
<p>#注意：在Sequential中，注意网络层的顺序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def test():</span><br><span class="line">    in_feature=50</span><br><span class="line">    out_feature=1</span><br><span class="line">    model=nn.Sequential(</span><br><span class="line">        nn.Linear(in_feature,256),</span><br><span class="line">        nn.Linear(256,128),</span><br><span class="line">        nn.Linear(128,out_feature)</span><br><span class="line">    )</span><br><span class="line">    print(model)</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    test()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Linear(in_features=50, out_features=256, bias=True)</span><br><span class="line">  (1): Linear(in_features=256, out_features=128, bias=True)</span><br><span class="line">  (2): Linear(in_features=128, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="2-1-3创建单层神经网络"><a href="#2-1-3创建单层神经网络" class="headerlink" title="2.1.3创建单层神经网络"></a>2.1.3创建单层神经网络</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def single_Linear():</span><br><span class="line">    model = nn.Linear(in_features=1,out_features=1)</span><br><span class="line">    return model</span><br></pre></td></tr></table></figure>

<h3 id="2-2优化器"><a href="#2-2优化器" class="headerlink" title="2.2优化器"></a>2.2优化器</h3><h4 id="2-2-1优化器的种类"><a href="#2-2-1优化器的种类" class="headerlink" title="2.2.1优化器的种类"></a>2.2.1优化器的种类</h4><p>SGD（随机梯度下降）</p>
<p>Adagrad（自适应梯度）</p>
<p>RMSprop（均方根传播）</p>
<p>Adam（自适应矩估计） </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer= optim.优化器名(model.parameters(),lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2应用案例"><a href="#2-2-2应用案例" class="headerlink" title="2.2.2应用案例"></a>2.2.2应用案例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    model = nn.Linear(<span class="number">20</span>,<span class="number">60</span>)</span><br><span class="line">    citerion = nn.MSELoss()</span><br><span class="line">    optimizer= optim.Adam(model.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line">    <span class="built_in">input</span>=torch.randn(<span class="number">128</span>,<span class="number">20</span>)</span><br><span class="line">    output=model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="comment">#计算损失值</span></span><br><span class="line">    loss=citerion(output,torch.randn(<span class="number">128</span>,<span class="number">60</span>))</span><br><span class="line">    <span class="comment">#梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment">#反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(loss.item())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;weight:&quot;</span>,model.weight)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;bias:&quot;</span>,model.bias)</span><br><span class="line">train()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1.3409423828125</span><br><span class="line">weight: Parameter containing:</span><br><span class="line">tensor([[ 0.0724,  0.1027, -0.0660,  ..., -0.0273, -0.0264, -0.1800],</span><br><span class="line">        [-0.0281,  0.1575,  0.1634,  ..., -0.0314,  0.1092, -0.0086],</span><br><span class="line">        [ 0.1279,  0.1842, -0.0106,  ..., -0.1463, -0.1246, -0.1302],</span><br><span class="line">        ...,</span><br><span class="line">        [-0.0610, -0.1233, -0.1233,  ..., -0.1522,  0.0029, -0.0435],</span><br><span class="line">        [ 0.1098,  0.2063, -0.1645,  ...,  0.1141,  0.0407, -0.0908],</span><br><span class="line">        [-0.0330, -0.0874, -0.0874,  ...,  0.0705,  0.0291,  0.0883]],</span><br><span class="line">       requires_grad=True)</span><br><span class="line">bias: Parameter containing:</span><br><span class="line">tensor([ 0.1164, -0.2069,  0.1480, -0.0183,  0.0230, -0.0850, -0.1558, -0.1516,</span><br><span class="line">        -0.1754,  0.0006, -0.0888, -0.1777, -0.1551,  0.0981, -0.0615, -0.1587,</span><br><span class="line">        -0.0701,  0.1460,  0.0986, -0.1845, -0.1084,  0.0571,  0.0190, -0.0688,</span><br><span class="line">         0.1900,  0.0007,  0.1982, -0.2084, -0.0538, -0.1474,  0.0202, -0.1793,</span><br><span class="line">         0.0878, -0.0778, -0.1341,  0.0501,  0.0746, -0.0751, -0.0039, -0.1991,</span><br><span class="line">         0.0469,  0.0513, -0.1663, -0.1727, -0.1498, -0.0621, -0.1859,  0.0028,</span><br><span class="line">        -0.1679,  0.0693,  0.0006, -0.0662, -0.1444,  0.1009, -0.2071, -0.1921,</span><br><span class="line">         0.0172, -0.2101,  0.1904,  0.0040], requires_grad=True)</span><br></pre></td></tr></table></figure>

<h4 id="2-2-3利用优化器优化代码"><a href="#2-2-3利用优化器优化代码" class="headerlink" title="2.2.3利用优化器优化代码"></a>2.2.3利用优化器优化代码</h4><p>优化前</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x=torch.tensor([1,3,5,6,7],dtype=torch.float32)</span><br><span class="line">y=4*x+2</span><br><span class="line">w=torch.tensor(1.0,requires_grad=True,dtype=torch.float32)</span><br><span class="line">b=torch.tensor(1.0,requires_grad=True,dtype=torch.float32)</span><br><span class="line">epochs=1500</span><br><span class="line">for i in range(epochs):</span><br><span class="line">    y_pred=w*x+b</span><br><span class="line">    loss=((y_pred-y)**2).mean()</span><br><span class="line">    loss.backward()</span><br><span class="line">    w.data=w.data-0.01*w.grad</span><br><span class="line">    b.data=b.data-0.01*b.grad</span><br><span class="line">    if i%10==0:</span><br><span class="line">        print(f&#x27;epoch:&#123;i&#125;,loss:&#123;loss.item()&#125;&#x27;)</span><br><span class="line">    w.grad=torch.tensor(0.0,requires_grad=True,dtype=torch.float32)</span><br><span class="line">    b.grad=torch.tensor(0.0,requires_grad=True,dtype=torch.float32)</span><br><span class="line">print(f&quot;参数为&#123;w&#125;,常数项为&#123;b&#125;&quot;)</span><br></pre></td></tr></table></figure>

<pre><code>w.data=w.data-0.01*w.grad
b.data=b.data-0.01*b.grad  ==》optimizer.step()
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w.grad=torch.tensor(0.0,requires_grad=True,dtype=torch.float32)</span><br><span class="line">    b.grad=torch.tensor(0.0,requires_grad=True,dtype=torch.float32)  ==》optimizer.zero_grad()</span><br></pre></td></tr></table></figure>

<p>优化后</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.utils.data import Dataset,DataLoader</span><br><span class="line">x=torch.tensor([[1],[3],[5],[6],[7]],dtype=torch.float32)</span><br><span class="line">y=4*x+2</span><br><span class="line">def test():</span><br><span class="line">    model = nn.Linear(1,1)</span><br><span class="line">    criterion = nn.MSELoss()</span><br><span class="line">    optimizer=optim.SGD(model.parameters(),lr=0.01)</span><br><span class="line">    epochs=2000</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        y_pred=model(x)</span><br><span class="line">        loss=criterion(y_pred,y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        if (epoch+1)%10==0:</span><br><span class="line">            print(f&#x27;epoch:&#123;epoch+1&#125;,loss:&#123;loss.item():.4f&#125;&#x27;)</span><br><span class="line">    print(model.weight.item())</span><br><span class="line">    print(model.bias.item())</span><br><span class="line"></span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    test()</span><br></pre></td></tr></table></figure>

<h2 id="3加载数据集"><a href="#3加载数据集" class="headerlink" title="3加载数据集"></a>3加载数据集</h2><h3 id="3-1加载数据的方法"><a href="#3-1加载数据的方法" class="headerlink" title="3.1加载数据的方法"></a>3.1加载数据的方法</h3><h4 id="3-1-1自定义加载数据"><a href="#3-1-1自定义加载数据" class="headerlink" title="3.1.1自定义加载数据"></a>3.1.1自定义加载数据</h4><p>步骤</p>
<p>#1.继承Dataset类<br>        #2.实现__init__()，初始化外部的数据<br>        #3.实现__len__()，返回数据集的长度<br>        #4.实现__getitem__()，根据索引获取对应位置的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载数据</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader,TensorDataset</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,data,labels</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.labels = labels</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        sample=<span class="variable language_">self</span>.data[index]</span><br><span class="line">        label=<span class="variable language_">self</span>.labels[index]</span><br><span class="line">        <span class="keyword">return</span> sample,label</span><br></pre></td></tr></table></figure>

<h4 id="3-1-2利用内置函数加载数据"><a href="#3-1-2利用内置函数加载数据" class="headerlink" title="3.1.2利用内置函数加载数据"></a>3.1.2利用内置函数加载数据</h4><p><strong>创建</strong> <strong>DataLoader</strong></p>
<p>dataloader &#x3D; DataLoader(<br>                   dataset,          # 数据集<br>                   batch_size&#x3D;10,    # 批量大小<br>                   shuffle&#x3D;True,     # 是否打乱数据<br>                   num_workers&#x3D;2     # 使用 2 个子进程加载数据<br>)</p>
<p>TensorDataset(samples,labels)</p>
<p>优先使用TensorDataset</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def test01():</span><br><span class="line">    x=torch.randn(100,20)#参数：样本数，特征数</span><br><span class="line">    y=torch.randn(100,1)</span><br><span class="line">    # dataset=MyDataset(x,y) 可以使用自定义的数据集，例如读取文件夹中的图片</span><br><span class="line">    dataset=TensorDataset(x,y)</span><br><span class="line">    print(dataset[0])</span><br><span class="line">    dataloader=DataLoader(dataset=dataset,</span><br><span class="line">                          batch_size=20,</span><br><span class="line">                          shuffle=True)</span><br><span class="line">    for sample,label in dataloader:</span><br><span class="line">        print(sample,label)</span><br><span class="line">        break</span><br></pre></td></tr></table></figure>

<h3 id="3-2读取各种数据"><a href="#3-2读取各种数据" class="headerlink" title="3.2读取各种数据"></a>3.2读取各种数据</h3><h4 id="3-2-1读取csv数据"><a href="#3-2-1读取csv数据" class="headerlink" title="3.2.1读取csv数据"></a>3.2.1读取csv数据</h4><p>步骤：1、利用pd读取csv文件,预处理数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line">df=pd.read_csv(<span class="string">r&#x27;D:\华清补习文件\第12章.深度学习\图片资料\大数据答辩成绩表.csv&#x27;</span>)</span><br><span class="line">df.head()</span><br><span class="line">df=df.drop([<span class="string">&#x27;学号&#x27;</span>,<span class="string">&#x27;姓名&#x27;</span>],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>2、创建类，将原始数据转化为tensor类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyCsvDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,df</span>):</span><br><span class="line">        data=torch.tensor(df.values)</span><br><span class="line">        <span class="variable language_">self</span>.data=data[:,:-<span class="number">1</span>]</span><br><span class="line">        <span class="variable language_">self</span>.label=data[:,-<span class="number">1</span>]</span><br><span class="line">        <span class="variable language_">self</span>.<span class="built_in">len</span>=<span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.<span class="built_in">len</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">        idx=<span class="built_in">min</span>(<span class="built_in">max</span>(idx,<span class="number">0</span>),<span class="variable language_">self</span>.<span class="built_in">len</span>-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx],<span class="variable language_">self</span>.label[idx]</span><br></pre></td></tr></table></figure>

<p>3、生成加载的数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">df</span>):</span><br><span class="line">    dataset=MyCsvDataset(df)</span><br><span class="line">    dataLoader=DataLoader(</span><br><span class="line">        dataset=dataset,</span><br><span class="line">        batch_size=<span class="number">12</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">    </span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(dataset))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> dataLoader:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>





<h4 id="3-2-2读取图片"><a href="#3-2-2读取图片" class="headerlink" title="3.2.2读取图片"></a>3.2.2读取图片</h4><p>api展示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, is_valid_file=None)</span><br></pre></td></tr></table></figure>

<p>参数解释</p>
<ul>
<li><code>root</code>：字符串，指定图像数据集的根目录。</li>
<li><code>transform</code>：可选参数，用于对图像进行预处理。通常是一个 <code>torchvision.transforms</code> 的组合。</li>
<li><code>target_transform</code>：可选参数，用于对目标（标签）进行转换。</li>
<li><code>is_valid_file</code>：可选参数，用于过滤无效文件。如果提供，只有返回 <code>True</code> 的文件才会被加载。</li>
</ul>
<p>步骤：1、创建transforms.compose作为工具对读入的图片进行预处理，</p>
<p>2、利用datasets中的ImageFolder()读取存放图片的类文件的目录</p>
<p>3、创建dataloader读取数据，可以利用循环输出dataloader的内容</p>
<p>路径打开后呈现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root/</span><br><span class="line">    class1/</span><br><span class="line">        img1.jpg</span><br><span class="line">        img2.jpg</span><br><span class="line">        ...</span><br><span class="line">    class2/</span><br><span class="line">        img1.jpg</span><br><span class="line">        img2.jpg</span><br><span class="line">        ...</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载图片数据集</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_img_data</span>():</span><br><span class="line">    transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">224</span>,<span class="number">224</span>)),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ])</span><br><span class="line">    path=<span class="string">r&#x27;D:\华清补习文件\第12章.深度学习\图片资料\animals&#x27;</span></span><br><span class="line">    dataset=datasets.ImageFolder(</span><br><span class="line">        root=path,</span><br><span class="line">        transform=transform<span class="comment">#transforms.Compose是组合</span></span><br><span class="line">    )</span><br><span class="line">    dataloader=DataLoader(</span><br><span class="line">        dataset=dataset,</span><br><span class="line">        batch_size=<span class="number">4</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line">        <span class="built_in">print</span>(y)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    load_img_data()</span><br></pre></td></tr></table></figure>



<h4 id="3-3-3加载官方数据集"><a href="#3-3-3加载官方数据集" class="headerlink" title="3.3.3加载官方数据集"></a>3.3.3加载官方数据集</h4><p>步骤：加载MINST数据集（MINST数据集是0到9的数据集，每一张图的尺寸为28*28）</p>
<p>参数：datasets.MNIST(root&#x3D;’code&#x2F;‘,train&#x3D;True,transform&#x3D;transforms.ToTensor(),download&#x3D;True)</p>
<p>root：存储数据集的本地路径</p>
<p>train：是否下载训练数据集，True：训练数据集，False：测试数据集</p>
<p>transform：转换器，可以将图片做预处理转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader,Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test01</span>():</span><br><span class="line">    dataset= datasets.MNIST(root=<span class="string">&#x27;code/&#x27;</span>,train=<span class="literal">True</span>,transform=transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">    dataloader=DataLoader(dataset,batch_size=<span class="number">4</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        <span class="built_in">print</span>(y.shape)</span><br><span class="line">        x=x.numpy()</span><br><span class="line">        cv.imshow(<span class="string">&#x27;img&#x27;</span>,x[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">        cv.waitKey(<span class="number">0</span>)</span><br><span class="line">        cv.destroyAllWindows()</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test01()</span><br><span class="line">    </span><br><span class="line">        </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">torch.Size([4])</span><br><span class="line">输出图片和数据的形状</span><br></pre></td></tr></table></figure>

<h2 id="4激活函数"><a href="#4激活函数" class="headerlink" title="4激活函数"></a>4激活函数</h2><h3 id="4-1sigmoid"><a href="#4-1sigmoid" class="headerlink" title="4.1sigmoid"></a>4.1sigmoid</h3><h4 id="4-1-1公式"><a href="#4-1-1公式" class="headerlink" title="4.1.1公式"></a>4.1.1公式</h4><p>Sigmoid函数的数学表达式为：<br>$$<br>f(x) &#x3D; \sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$<br>其中，$$e$$ 是自然常数（约等于2.718），$$x$$ 是输入。</p>
<h4 id="4-1-2特点"><a href="#4-1-2特点" class="headerlink" title="4.1.2特点"></a>4.1.2特点</h4><p>能够将输入的数据映射到（0,1）之间，适合处理概率问题</p>
<p>一般用于二分类</p>
<p>微分性质：$$ \sigma’(x)&#x3D;\sigma(x)\cdot(1-\sigma(x)) $$</p>
<h4 id="4-1-3缺点"><a href="#4-1-3缺点" class="headerlink" title="4.1.3缺点"></a>4.1.3缺点</h4><p>梯度消失：在输入非常大或者非常小的值时，sigmoid函数的梯度会变得非常小且接近于0，导致在反向传播过程中国，梯度逐渐衰减</p>
<p>信息损失：输入100和更大的值映射后的值没有差别</p>
<p>计算复杂</p>
<h3 id="4-2tanh"><a href="#4-2tanh" class="headerlink" title="4.2tanh"></a>4.2tanh</h3><h4 id="4-2-1公式"><a href="#4-2-1公式" class="headerlink" title="4.2.1公式"></a>4.2.1公式</h4><p>tanh数学表达式为：<br>$$<br>{tanh}(x) &#x3D; \frac{e^x - e^{-x}}{e^x + e^{-x}}<br>$$</p>
<h4 id="4-2-2特点"><a href="#4-2-2特点" class="headerlink" title="4.2.2特点"></a>4.2.2特点</h4><p>输出范围：可将数据映射到（-1,1），因此输出的是零中心，相比于Sigmoid函数，这种零中心化的输出有助于加速收敛</p>
<p>对称性：输入0输出也是0，使数据平衡</p>
<h4 id="4-2-3缺点"><a href="#4-2-3缺点" class="headerlink" title="4.2.3缺点"></a>4.2.3缺点</h4><p>梯度消失：与sigmoid相似，输入非常大活非常小，没进入下一层就会乘以一个比一小的数，导致梯度变小，训练过程变得非常慢，甚至无法收敛</p>
<p>计算成本略高</p>
<h3 id="4-3Relu"><a href="#4-3Relu" class="headerlink" title="4.3Relu"></a>4.3Relu</h3><h4 id="4-3-1公式"><a href="#4-3-1公式" class="headerlink" title="4.3.1公式"></a>4.3.1公式</h4><p>ReLU 函数定义如下：<br>$$<br>\text{ReLU}(x) &#x3D; \max(0, x)<br>$$</p>
<p>即$$ReLU$$对输入$$x$$进行非线性变换：<br>$$<br>\bullet\quad\text{当 }x&gt;0\text{ 时,ReLU}(x)&#x3D;x\text{}\\bullet\quad\text{当 }x\leq0\text{ 时,ReLU}(x)&#x3D;0\text{}<br>$$</p>
<h4 id="4-2-2特点-1"><a href="#4-2-2特点-1" class="headerlink" title="4.2.2特点"></a>4.2.2特点</h4><p>计算简单，只需要进行比较运算</p>
<p>可以缓解梯度消失的问题，Relu的导数恒为1，不存在梯度变的极小导致无法收敛的情况</p>
<p>稀疏激活：当输入小于0时输出为0一些神经元不被激活，减少网络中的冗余信息</p>
<h4 id="4-2-3缺点-1"><a href="#4-2-3缺点-1" class="headerlink" title="4.2.3缺点"></a>4.2.3缺点</h4><p>神经元死亡：减少模型的表达能力</p>
<h3 id="4-4-LeakyReLU"><a href="#4-4-LeakyReLU" class="headerlink" title="4.4 LeakyReLU"></a>4.4 LeakyReLU</h3><h4 id="4-4-1公式"><a href="#4-4-1公式" class="headerlink" title="4.4.1公式"></a>4.4.1公式</h4><p>Leaky ReLU 函数的定义如下：<br>$$<br>\text{Leaky ReLU}(x)&#x3D;\begin{cases}x,&amp;\text{if } x&gt;0\\alpha x,&amp;\text{if } x\leq0\end{cases}<br>$$<br>其中，$$\alpha$$ 是一个非常小的常数（如 0.01），它控制负半轴的斜率。这个常数 $$\alpha$$是一个超参数，可以在训练过程中可自行进行调整。</p>
<h4 id="4-4-2特点"><a href="#4-4-2特点" class="headerlink" title="4.4.2特点"></a>4.4.2特点</h4><p>避免神经元死亡：引入一个极小的$\alpha$当输入小于0时，改进后的relu仍然有梯度，允许其继续更新权重。</p>
<p>计算简单</p>
<h4 id="4-4-3缺点"><a href="#4-4-3缺点" class="headerlink" title="4.4.3缺点"></a>4.4.3缺点</h4><ol>
<li>参数选择：$$\alpha$$ 是一个需要调整的超参数，选择合适的$$\alpha$$ 值可能需要实验和调优。</li>
<li>出现负激活：如果$$\alpha$$ 设定得不当，仍然可能导致激活值过低。</li>
</ol>
<h3 id="4-5softmax"><a href="#4-5softmax" class="headerlink" title="4.5softmax"></a>4.5softmax</h3><h4 id="4-5-1公式"><a href="#4-5-1公式" class="headerlink" title="4.5.1公式"></a>4.5.1公式</h4><p>假设神经网络的输出层有$$n$$个节点，每个节点的输入为$$z_i$$，则 Softmax 函数的定义如下：<br>$$<br>\mathrm{Softmax}(z_i)&#x3D;\frac{e^{z_i}}{\sum_{j&#x3D;1}^ne^{z_j}}<br>$$</p>
<p>给定输入向量 $z&#x3D;[z_1,z_2,…,z_n]$</p>
<p>1.指数变换：对每个 $z_i$进行指数变换，得到 $t &#x3D; [e^{z_1},e^{z_2},…,e^{z_n}]$，使z的取值区间从$(-\infty,+\infty)$变为$(0,+\infty)$</p>
<p>2.将所有指数变换后的值求和，得到$s &#x3D; e^{z_1} + e^{z_2} + … + e^{z_n} &#x3D; \Sigma_{j&#x3D;1}^ne^{z_j}$</p>
<p>3.将t中每个 $e^{z_i}$除以归一化因子s，得到概率分布:<br>$$<br>softmax(z) &#x3D;[\frac{e^{z_1}}{s},\frac{e^{z_2}}{s},…,\frac{e^{z_n}}{s}]&#x3D;[\frac{e^{z_1}}{\Sigma_{j&#x3D;1}^ne^{z_j}},\frac{e^{z_2}}{\Sigma_{j&#x3D;1}^ne^{z_j}},…,\frac{e^{z_n}}{\Sigma_{j&#x3D;1}^ne^{z_j}}]<br>$$<br>即：<br>$$<br>\mathrm{Softmax}(z_i)&#x3D;\frac{e^{z_i}}{\sum_{j&#x3D;1}^ne^{z_j}}<br>$$<br>从上述公式可以看出：</p>
<ol>
<li><p>每个输出值在 (0,1)之间</p>
</li>
<li><p>Softmax()对向量的值做了改变，但其位置不变</p>
</li>
<li><p>所有输出值之和为1，即</p>
</li>
</ol>
<p>$$<br>sum(softmax(z)) &#x3D;\frac{e^{z_1}}{s}+\frac{e^{z_2}}{s}+…+\frac{e^{z_n}}{s}&#x3D;\frac{s}{s}&#x3D;1<br>$$</p>
<h4 id><a href="#" class="headerlink" title></a></h4><h4 id="4-5-2特点"><a href="#4-5-2特点" class="headerlink" title="4.5.2特点"></a>4.5.2特点</h4><p>将输入转化为概率</p>
<p>概率分布：输出值介于0到1之间</p>
<p>突出差异</p>
<h4 id="4-5-3缺点"><a href="#4-5-3缺点" class="headerlink" title="4.5.3缺点"></a>4.5.3缺点</h4><p>数值不稳定，输入值过大导致$e^{z_i}$</p>
<p>难以处理大量类别：计算量巨量增加</p>
<h2 id="5参数初始化"><a href="#5参数初始化" class="headerlink" title="5参数初始化"></a>5参数初始化</h2><h3 id="5-1固定值初始化"><a href="#5-1固定值初始化" class="headerlink" title="5.1固定值初始化"></a>5.1固定值初始化</h3><h4 id="5-1-1-全零初始化"><a href="#5-1-1-全零初始化" class="headerlink" title="5.1.1 全零初始化"></a>5.1.1 全零初始化</h4><p>将神经网络中的所有权重参数初始化为0。</p>
<p><strong>方法</strong>：将所有权重初始化为零。</p>
<p><strong>缺点</strong>：导致对称性破坏，每个神经元在每一层中都会执行相同的计算，模型无法学习。</p>
<p><strong>应用场景</strong>：通常不用来初始化权重，但可以用来初始化偏置。</p>
<p><strong>对称性问题</strong></p>
<ul>
<li><p>现象：同一层的所有神经元具有完全相同的初始权重和偏置。</p>
</li>
<li><p>后果：</p>
<ul>
<li><p>在反向传播时，所有神经元会收到相同的梯度，导致权重更新完全一致。</p>
</li>
<li><p>无论训练多久，同一层的神经元本质上会保持相同的功能（相当于“一个神经元”的多个副本），极大降低模型的表达能力。</p>
</li>
<li><p>nn.init.zeros_(模型名.weight)</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test001():</span><br><span class="line">    # 1. 均匀分布随机初始化</span><br><span class="line">    linear = nn.Linear(in_features=6, out_features=4)</span><br><span class="line">    # 初始化权重参数</span><br><span class="line">    nn.init.zeros_(linear.weight)</span><br><span class="line">    # 打印权重参数</span><br><span class="line">    print(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    test001()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0.]], requires_grad=True)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="5-1-2全1初始化"><a href="#5-1-2全1初始化" class="headerlink" title="5.1.2全1初始化"></a>5.1.2全1初始化</h4><p>不经常使用</p>
<p> nn.init.ones_(,model名.weight)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test001():</span><br><span class="line">    # 1. 均匀分布随机初始化</span><br><span class="line">    linear = nn.Linear(in_features=6, out_features=4)</span><br><span class="line">    # 初始化权重参数</span><br><span class="line">    nn.init.ones_(linear.weight)</span><br><span class="line">    # 打印权重参数</span><br><span class="line">    print(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    test001()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[1., 1., 1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1., 1., 1.]], requires_grad=True)</span><br></pre></td></tr></table></figure>

<h4 id="5-1-3任意常数初始化"><a href="#5-1-3任意常数初始化" class="headerlink" title="5.1.3任意常数初始化"></a>5.1.3任意常数初始化</h4><p>将所有参数初始为某个非零的常数</p>
<p> nn.init.constant_(模型.weight,设定的值)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test001():</span><br><span class="line">    # 1. 均匀分布随机初始化</span><br><span class="line">    linear = nn.Linear(in_features=6, out_features=4)</span><br><span class="line">    # 初始化权重参数</span><br><span class="line">    nn.init.constant_(linear.weight,0.63)</span><br><span class="line">    # 打印权重参数</span><br><span class="line">    print(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    test001()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[0.6300, 0.6300, 0.6300, 0.6300, 0.6300, 0.6300],</span><br><span class="line">        [0.6300, 0.6300, 0.6300, 0.6300, 0.6300, 0.6300],</span><br><span class="line">        [0.6300, 0.6300, 0.6300, 0.6300, 0.6300, 0.6300],</span><br><span class="line">        [0.6300, 0.6300, 0.6300, 0.6300, 0.6300, 0.6300]], requires_grad=True)</span><br></pre></td></tr></table></figure>

<h3 id="5-2随机初始化"><a href="#5-2随机初始化" class="headerlink" title="5.2随机初始化"></a>5.2随机初始化</h3><p><strong>方法</strong>：将权重初始化为随机的小值，通常从正态分布或均匀分布中采样。</p>
<p><strong>应用场景</strong>：这是最基本的初始化方法，通过随机初始化避免对称性破坏。</p>
<p>代码演示：随机分布之均匀初始化</p>
<p>nn.init.uniform_(模型.weight)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test001():</span><br><span class="line">    # 1. 均匀分布随机初始化</span><br><span class="line">    linear = nn.Linear(in_features=6, out_features=4)</span><br><span class="line">    # 初始化权重参数</span><br><span class="line">    nn.init.uniform_(linear.weight)</span><br><span class="line">    # 打印权重参数</span><br><span class="line">    print(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    test001()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="5-3Xavier"><a href="#5-3Xavier" class="headerlink" title="5.3Xavier"></a>5.3Xavier</h3><p>优点**：平衡了输入和输出的方差，适合$$Sigmoid$$ 和 $$Tanh$$ 激活函数。</p>
<p><strong>应用场景</strong>：常用于浅层网络或使用$$Sigmoid$$ 、$$Tanh$$ 激活函数的网络。</p>
<p> nn.init.xavier_normal_(模型.weight)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test007():</span><br><span class="line">    # Xavier初始化：正态分布</span><br><span class="line">    linear = nn.Linear(in_features=6, out_features=4)</span><br><span class="line">    nn.init.xavier_normal_(linear.weight)</span><br><span class="line">    print(linear.weight)</span><br><span class="line"></span><br><span class="line">    # Xavier初始化：均匀分布</span><br><span class="line">    linear = nn.Linear(in_features=6, out_features=4)</span><br><span class="line">    nn.init.xavier_uniform_(linear.weight)</span><br><span class="line">    print(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    test007()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="5-4He初始化"><a href="#5-4He初始化" class="headerlink" title="5.4He初始化"></a>5.4He初始化</h3><p><strong>优点</strong>：适用于$$ReLU$$ 和 $$Leaky ReLU$$ 激活函数。</p>
<p><strong>应用场景</strong>：深度网络，尤其是使用 ReLU 激活函数时。</p>
<p>nn.init.kaiming_normal_(linear.weight, nonlinearity&#x3D;”relu”, mode&#x3D;’fan_in’)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test006():</span><br><span class="line">    # He初始化：正态分布</span><br><span class="line">    linear = nn.Linear(in_features=6, out_features=4)</span><br><span class="line">    nn.init.kaiming_normal_(linear.weight, nonlinearity=&quot;relu&quot;, mode=&#x27;fan_in&#x27;)</span><br><span class="line">    print(linear.weight)</span><br><span class="line"></span><br><span class="line">    # He初始化：均匀分布</span><br><span class="line">    linear = nn.Linear(in_features=6, out_features=4)</span><br><span class="line">    nn.init.kaiming_uniform_(linear.weight, nonlinearity=&quot;relu&quot;, mode=&#x27;fan_out&#x27;)</span><br><span class="line">    print(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    test006()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="5-5总结"><a href="#5-5总结" class="headerlink" title="5.5总结"></a>5.5总结</h3><p>在使用Torch构建网络模型时，每个网络层的参数都有默认的初始化方法，同时还可以通过以上方法来对网络参数进行初始化。</p>
<h2 id="6损失函数"><a href="#6损失函数" class="headerlink" title="6损失函数"></a>6损失函数</h2><h3 id="6-1线性回归损失函数"><a href="#6-1线性回归损失函数" class="headerlink" title="6.1线性回归损失函数"></a>6.1线性回归损失函数</h3><h4 id="6-1-1MAE损失"><a href="#6-1-1MAE损失" class="headerlink" title="6.1.1MAE损失"></a>6.1.1MAE损失</h4><p>MAE（Mean Absolute Error，平均绝对误差）通常也被称为 L1-Loss，通过对预测值和真实值之间的绝对差取平均值来衡量他们之间的差异。</p>
<p>MAE的公式如下：<br>$$<br>\text{MAE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left| y_i - \hat{y}_i \right|<br>$$<br>其中：</p>
<ul>
<li>$$n$$ 是样本的总数。</li>
<li>$$ y_i $$ 是第 $$i$$ 个样本的真实值。</li>
<li>$$ \hat{y}_i$$ 是第 $$i$$ 个样本的预测值。</li>
<li>$$\left| y_i - \hat{y}_i \right|$$ 是真实值和预测值之间的绝对误差。</li>
</ul>
<p><strong>特点</strong>：</p>
<ol>
<li><strong>鲁棒性</strong>：与均方误差（MSE）相比，MAE对异常值（outliers）更为鲁棒，因为它不会像MSE那样对较大误差平方敏感。</li>
<li><strong>物理意义直观</strong>：MAE以与原始数据相同的单位度量误差，使其易于解释。</li>
</ol>
<p><strong>应用场景</strong>：<br>MAE通常用于需要对误差进行线性度量的情况，尤其是当数据中可能存在异常值时，MAE可以避免对异常值的过度惩罚。</p>
<p>使用<code>torch.nn.L1Loss</code>即可计算MAE：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化MAE损失函数</span></span><br><span class="line">mae_loss = nn.L1Loss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 y_true 是真实值, y_pred 是预测值</span></span><br><span class="line">y_true = torch.tensor([<span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">2.5</span>])</span><br><span class="line">y_pred = torch.tensor([<span class="number">2.5</span>, <span class="number">5.0</span>, <span class="number">3.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算MAE</span></span><br><span class="line">loss = mae_loss(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;MAE Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="6-1-2MSE损失"><a href="#6-1-2MSE损失" class="headerlink" title="6.1.2MSE损失"></a>6.1.2MSE损失</h4><p>均方差损失，也叫L2Loss。</p>
<p>MSE（Mean Squared Error，均方误差）通过对预测值和真实值之间的误差平方取平均值，来衡量预测值与真实值之间的差异。</p>
<p>MSE的公式如下：<br>$$<br>\text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left( y_i - \hat{y}_i \right)^2<br>$$<br>其中：</p>
<ul>
<li>$$n$$ 是样本的总数。</li>
<li>$$ y_i $$ 是第 $$i$$ 个样本的真实值。</li>
<li>$$ \hat{y}_i $$ 是第 $$i$$ 个样本的预测值。</li>
<li>$$\left( y_i - \hat{y}_i \right)^2$$ 是真实值和预测值之间的误差平方。</li>
</ul>
<p><strong>特点</strong>：</p>
<ol>
<li>平方惩罚：因为误差平方，MSE 对较大误差施加更大惩罚，所以 MSE 对异常值更为敏感。</li>
<li>凸性：MSE 是一个凸函数(国际的叫法，国内叫凹函数)，这意味着它具有一个唯一的全局最小值，有助于优化问题的求解。</li>
</ol>
<p><strong>应用场景</strong>：</p>
<p>MSE被广泛应用在神经网络中。</p>
<p>使用 torch.nn.MSELoss 可以实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化MSE损失函数</span></span><br><span class="line">mse_loss = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 y_true 是真实值, y_pred 是预测值</span></span><br><span class="line">y_true = torch.tensor([<span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">2.5</span>])</span><br><span class="line">y_pred = torch.tensor([<span class="number">2.5</span>, <span class="number">5.0</span>, <span class="number">3.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算MSE</span></span><br><span class="line">loss = mse_loss(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;MSE Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="6-2交叉熵损失函数"><a href="#6-2交叉熵损失函数" class="headerlink" title="6.2交叉熵损失函数"></a>6.2交叉熵损失函数</h2><h3 id="6-2-1-信息量"><a href="#6-2-1-信息量" class="headerlink" title="6.2.1 信息量"></a>6.2.1 信息量</h3><p>信息量用于衡量一个事件所包含的信息的多少。信息量的定义基于事件发生的概率：事件发生的概率越低，其信息量越大。其量化公式：</p>
<p>对于一个事件x，其发生的概率为 P(x)，信息量I(x) 定义为：<br>$$<br>I(x)&#x3D;−logP(x)<br>$$<br>性质</p>
<ol>
<li>非负性：I(x)≥0。</li>
<li>单调性：P(x)越小，I(x)越大。</li>
</ol>
<h4 id="6-2-2信息熵"><a href="#6-2-2信息熵" class="headerlink" title="6.2.2信息熵"></a>6.2.2信息熵</h4><p>信息熵是信息量的期望值。熵越高，表示随机变量的不确定性越大；熵越低，表示随机变量的不确定性越小。</p>
<p>公式由数学中的期望推导而来：<br>$$<br>H(X)&#x3D;−∑_{i&#x3D;1}^n P(x_i)logP(x_i)<br>$$<br>其中：</p>
<p>$-logP(x_i)$是信息量，$P(x_i)$是信息量对应的概率</p>
<h4 id="6-2-3KL散度"><a href="#6-2-3KL散度" class="headerlink" title="6.2.3KL散度"></a>6.2.3KL散度</h4><p>KL散度用于衡量两个概率分布之间的差异。它描述的是用一个分布 Q来近似另一个分布 P时，所损失的信息量。KL散度越小，表示两个分布越接近。</p>
<p>对于两个离散概率分布 P和 Q，KL散度定义为：<br>$$<br>D_{KL}(P||Q)&#x3D;∑_iP(x_i)log\frac{⁡P(x_i)}{Q(x_i)}<br>$$<br>其中：<em>P</em> 是真实分布，Q是近似分布。</p>
<h4 id="6-2-4交叉熵"><a href="#6-2-4交叉熵" class="headerlink" title="6.2.4交叉熵"></a>6.2.4交叉熵</h4><p>对KL散度公式展开：<br>$$<br>D_{KL}(P||Q)&#x3D;∑_iP(x_i)log\frac{⁡P(x_i)}{Q(x_i)}&#x3D;∑_iP(x_i)[log{⁡P(x_i)}-log{Q(x_i)}]\<br>&#x3D;∑_iP(x_i)log{⁡P(x_i)}-∑_iP(x_i)log{Q(x_i)}&#x3D;-(-∑_iP(x_i)log{⁡P(x_i)})+(-∑_iP(x_i)log{Q(x_i)})\<br>&#x3D;-H(P)+(-∑_iP(x_i)log{Q(x_i)})\<br>&#x3D;H(P,Q)-H(P)<br>$$<br>由上述公式可知，P是真实分布，H(P)是常数，所以KL散度可以用H(P,Q)来表示；H(P,Q)叫做交叉熵。</p>
<p>如果将P换成y，Q换成$\hat{y}$,则交叉熵公式为：<br>$$<br>\text{CrossEntropyLoss}(y, \hat{y}) &#x3D; - \sum_{i&#x3D;1}^{C} y_i \log(\hat{y}_i)<br>$$</p>
<p>其中：</p>
<ul>
<li>$$C$$ 是类别的总数。</li>
<li>$$ y $$ 是真实标签的one-hot编码向量，表示真实类别。</li>
<li>$$\hat{y}$$ 是模型的输出（经过 softmax 后的概率分布）。</li>
<li>$$y_i$$ 是真实类别的第 $$i$$ 个元素（0 或 1）。</li>
<li>$$ \hat{y}_i $$ 是预测的类别概率分布中对应类别 $$i$$ 的概率。</li>
</ul>
<p><strong>特点：</strong></p>
<ol>
<li><p>概率输出：CrossEntropyLoss 通常与 softmax 函数一起使用，使得模型的输出表示为一个概率分布（即所有类别的概率和为 1）。PyTorch 的 nn.CrossEntropyLoss 已经<strong>内置了 Softmax 操作</strong>。如果我们在输出层显式地添加 Softmax，会导致重复应用 Softmax，从而影响模型的训练效果。</p>
</li>
<li><p>惩罚错误分类：该损失函数在真实类别的预测概率较低时，会施加较大的惩罚，这样模型在训练时更注重提升正确类别的预测概率。</p>
</li>
<li><p>多分类问题中的标准选择：在大多数多分类问题中，CrossEntropyLoss 是首选的损失函数。</p>
</li>
</ol>
<p><strong>应用场景：</strong></p>
<p>CrossEntropyLoss 广泛应用于各种分类任务，包括图像分类、文本分类等，尤其是在神经网络模型中。</p>
<p>nn.CrossEntropyLoss基本原理：</p>
<p>由交叉熵公式可知：<br>$$<br>\text{Loss}(y, \hat{y}) &#x3D; - \sum_{i&#x3D;1}^{C} y_i \log(\hat{y}_i)<br>$$<br>因为$y_i$是one-hot编码，其值不是1便是0，又是乘法，所以只要知道1对应的index就可以了，展开后：<br>$$<br>\text{Loss}(y, \hat{y}) &#x3D; - \log(\hat{y}_m)<br>$$<br>其中，m表示真实类别。</p>
<p>因为神经网络最后一层分类总是接softmax，所以可以把$\hat{y}_m$直接看为是softmax后的结果。<br>$$<br>\text{Loss}(i) &#x3D; - \log(softmax(x_i))<br>$$<br>所以，<code>CrossEntropyLoss</code> 实质上是两步的组合：Cross Entropy &#x3D; Log-Softmax + NLLLoss</p>
<ul>
<li><strong>Log-Softmax</strong>：对输入 logits 先计算对数 softmax：<code>log(softmax(x))</code>。</li>
<li><strong>NLLLoss（Negative Log-Likelihood）</strong>：对 log-softmax 的结果计算负对数似然损失。简单理解就是求负数。原因是概率值通常在 0 到 1 之间，取对数后会变成负数。为了使损失值为正数，需要取负数。</li>
</ul>
<p>对于$softmax(x_i)$，在softmax介绍中了解到，需要减去最大值以确保数值稳定。<br>$$<br>\mathrm{Softmax}(x_i)&#x3D;\frac{e^{x_i-\max(x)}}{\sum_{j&#x3D;1}^ne^{x_j-\max(x)}}<br>$$<br>则：<br>$$<br>LogSoftmax(x_i) &#x3D;log(\frac{e^{x_i-\max(x)}}{\sum_{j&#x3D;1}^ne^{x_j-\max(x)}})\<br>&#x3D;x_i-\max(x)-log(\sum_{j&#x3D;1}^ne^{x_j-\max(x)})<br>$$<br>所以：<br>$$<br>\text{Loss}(i) &#x3D; - (x_i-\max(x)-log(\sum_{j&#x3D;1}^ne^{x_j-\max(x)}))<br>$$<br>总的交叉熵损失函数是所有样本的平均值：<br>$$<br>\ell(x, y) &#x3D; \begin{cases}<br>              \frac{\sum_{n&#x3D;1}^N l_n}{N}, &amp;<br>               \text{if reduction} &#x3D; \text{<code>mean&#39;;&#125;\\                 \sum_&#123;n=1&#125;^N l_n,  &amp;                 \text&#123;if reduction&#125; = \text&#123;</code>sum’.}<br>            \end{cases}<br>$$<br>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设有三个类别，模型输出是未经softmax的logits</span></span><br><span class="line">logits = torch.tensor([[<span class="number">1.5</span>, <span class="number">2.0</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">1.5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实的标签</span></span><br><span class="line">labels = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])  <span class="comment"># 第一个样本的真实类别为1，第二个样本的真实类别为2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化CrossEntropyLoss</span></span><br><span class="line"><span class="comment"># 参数：reduction：mean-平均值，sum-总和</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(logits, labels)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Cross Entropy Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="7优化器"><a href="#7优化器" class="headerlink" title="7优化器"></a>7优化器</h2><h3 id="7-1Momentum"><a href="#7-1Momentum" class="headerlink" title="7.1Momentum"></a>7.1Momentum</h3><p>动量（Momentum）是对梯度下降的优化方法，可以更好地应对梯度变化和梯度消失问题，从而提高训练模型的效率和稳定性。它通过引入 <strong>指数加权平均</strong> 来积累历史梯度信息，从而在更新参数时形成“动量”，帮助优化算法更快地越过局部最优或鞍点。</p>
<p>梯度更新算法包括<strong>两个步骤：</strong></p>
<p>a. 更新动量项</p>
<p>首先计算当前的动量项 $$v_t$$： $$v_{t} &#x3D; \beta v_{t-1} + (1 - \beta) \nabla_\theta J(\theta_t)$$ 其中：</p>
<ul>
<li>$$v_{t-1}$$ 是之前的动量项；</li>
<li>$$\beta$$ 是动量系数（通常为 0.9）；</li>
<li>$$\nabla_\theta J(\theta_t)$$ 是当前的梯度；</li>
</ul>
<p>b. 更新参数</p>
<p>利用动量项更新参数：<br>$$<br>v_{t}&#x3D;\beta v_{t-1}+(1-\beta)\nabla_\theta J(\theta_t)<br>\<br>\theta_{t}&#x3D;\theta_{t-1}-\eta v_{t}<br>$$<br><strong>特点</strong>：</p>
<ul>
<li><p><strong>惯性效应：</strong> 该方法加入前面梯度的累积，这种惯性使得算法沿着当前的方向继续更新。如遇到鞍点，也不会因梯度逼近零而停滞。</p>
</li>
<li><p><strong>减少震荡：</strong> 该方法平滑了梯度更新，减少在鞍点附近的震荡，帮助优化过程稳定向前推进。</p>
</li>
<li><p><strong>加速收敛：</strong> 该方法在优化过程中持续沿着某个方向前进，能够更快地穿越鞍点区域，避免在鞍点附近长时间停留。</p>
</li>
</ul>
<p><strong>在方向上的作用</strong>：</p>
<p>（1）梯度方向一致时</p>
<ul>
<li>如果梯度在多个连续时刻方向一致（例如，一直指向某个方向），Momentum 会逐渐积累动量，使更新速度加快。</li>
<li>例如，假设梯度在多个时刻都是正向的，动量 $v_t$ 会逐渐增大，从而加速参数更新。</li>
</ul>
<p>（2）梯度方向不一致时</p>
<ul>
<li>如果梯度方向在不同时刻不一致（例如，来回震荡），Momentum 会通过积累的历史梯度信息部分抵消这些震荡。</li>
<li>例如，假设梯度在一个时刻是正向的，下一个时刻是负向的，动量 $v_t$ 会平滑这些变化，使更新路径更加稳定。</li>
</ul>
<p>（3）局部最优或鞍点附近</p>
<ul>
<li>在局部最优或鞍点附近，梯度可能会变得很小，导致标准梯度下降法停滞。</li>
<li>Momentum 通过积累历史梯度信息，可以帮助参数更新越过这些平坦区域。</li>
</ul>
<p><strong>动量方向与梯度方向一致</strong></p>
<p>（1）梯度方向一致时</p>
<ul>
<li>如果梯度在多个连续时刻方向一致（例如，一直指向某个方向），动量会逐渐积累，动量方向与梯度方向一致。</li>
<li>例如，假设梯度在多个时刻都是正向的，动量 $v_t$ 会逐渐增大，从而加速参数更新。</li>
</ul>
<p>（2）<strong>几何意义</strong></p>
<ul>
<li>在优化问题中，如果损失函数的几何形状是 <strong>平滑且单调</strong> 的（例如，一个狭长的山谷），梯度方向会保持一致。</li>
<li>在这种情况下，动量方向与梯度方向一致，Momentum 会加速参数更新，帮助算法更快地收敛。</li>
</ul>
<p><strong>动量方向与梯度方向不一致</strong></p>
<p>（1）梯度方向不一致时</p>
<ul>
<li>如果梯度方向在不同时刻不一致（例如，来回震荡），动量方向可能会与当前梯度方向不一致。</li>
<li>例如，假设梯度在一个时刻是正向的，下一个时刻是负向的，动量 $v_t$ 会平滑这些变化，使更新路径更加稳定。</li>
</ul>
<p>（2）几何意义</p>
<ul>
<li>在优化问题中，如果损失函数的几何形状是 <strong>复杂且非凸</strong> 的（例如，存在多个局部最优或鞍点），梯度方向可能会在不同时刻发生剧烈变化。</li>
<li>在这种情况下，动量方向与梯度方向可能不一致，Momentum 会通过积累的历史梯度信息部分抵消这些震荡，使更新路径更加平滑。</li>
</ul>
<p><strong>总结</strong>：</p>
<ul>
<li>动量项更新：利用当前梯度和历史动量来计算新的动量项。</li>
<li>权重参数更新：利用更新后的动量项来调整权重参数。</li>
<li>梯度计算：在每个时间步计算当前的梯度，用于更新动量项和权重参数。</li>
</ul>
<p>Momentum 算法是对梯度值的平滑调整，但是<strong>并没有</strong>对梯度下降中的学习率进行优化。</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型和损失函数</span></span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># PyTorch 中 momentum 直接作为参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据</span></span><br><span class="line">X = torch.randn(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">y = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = model(X)</span><br><span class="line">    loss = criterion(outputs, y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="7-2AdaGrad"><a href="#7-2AdaGrad" class="headerlink" title="7.2AdaGrad"></a>7.2AdaGrad</h3><p>AdaGrad（Adaptive Gradient Algorithm）为每个参数引入独立的学习率，它根据历史梯度的平方和来调整这些学习率。具体来说，对于频繁更新的参数，其学习率会逐渐减小；而对于更新频率较低的参数，学习率会相对较大。AdaGrad避免了统一学习率的不足，更多用于处理稀疏数据和梯度变化较大的问题。</p>
<p>AdaGrad流程：</p>
<ol>
<li><p><strong>初始化</strong>：</p>
<ul>
<li>初始化参数 $$ \theta_0 $$ 和学习率 $$ \eta $$。</li>
<li>将梯度累积平方的向量 $$ G_0$$ 初始化为零向量。</li>
</ul>
</li>
<li><p><strong>梯度计算</strong>：</p>
<ul>
<li>在每个时间步 $$t$$，计算损失函数$$ J(\theta)$$对参数 $$\theta$$ 的梯度$$ g_t &#x3D; \nabla_\theta J(\theta_t)$$。</li>
</ul>
</li>
<li><p><strong>累积梯度的平方</strong>：</p>
<ul>
<li><p>对每个参数 $$ i $$累积梯度的平方：<br>$$<br>G_{t} &#x3D; G_{t-1} + g_{t}^2\<br>$$<br>其中$$G_{t}$$ 是累积的梯度平方和，$$ g_{t}$$ 是第$$ i $$个参数在时间步$$t$$ 的梯度。</p>
<p>推导：<br>$$<br>G_{t} &#x3D; G_{t-1} + g_{t}^2&#x3D;G_{t-2} + g_{t-1}^2 + g_{t}^2 &#x3D; … &#x3D; g_{1}^2 + … + g_{t-1}^2 + g_{t}^2<br>$$</p>
</li>
</ul>
</li>
<li><p><strong>参数更新</strong>：</p>
<ul>
<li><p>利用累积的梯度平方来更新参数：<br>$$<br>\theta_{t} &#x3D; \theta_{t-1} - \frac{\eta}{\sqrt{G_{t} + \epsilon}} g_{t}<br>$$</p>
</li>
<li><p>其中：</p>
<ul>
<li>$$\eta$$ 是全局的初始学习率。</li>
<li>$$ \epsilon$$ 是一个非常小的常数，用于避免除零操作（通常取$$ 10^{-8}$$）。</li>
<li>$$ \frac{\eta}{\sqrt{G_{t} + \epsilon}} $$ 是自适应调整后的学习率。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>AdaGrad 为每个参数分配不同的学习率：</p>
<ul>
<li>对于梯度较大的参数，Gt较大，学习率较小，从而避免更新过快。</li>
<li>对于梯度较小的参数，Gt较小，学习率较大，从而加快更新速度。</li>
</ul>
<p>可以将 AdaGrad 类比为：</p>
<ul>
<li>梯度较大的参数：类似于陡峭的山坡，需要较小的步长（学习率）以避免跨度过大。</li>
<li>梯度较小的参数：类似于平缓的山坡，可以采取较大的步长（学习率）以加快收敛。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>自适应学习率：由于每个参数的学习率是基于其梯度的累积平方和 $$ G_{t,i}$$ 来动态调整的，这意味着学习率会随着时间步的增加而减少，对梯度较大且变化频繁的方向非常有用，防止了梯度过大导致的震荡。</li>
<li>适合稀疏数据：AdaGrad 在处理稀疏数据时表现很好，因为它能够自适应地为那些较少更新的参数保持较大的学习率。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ol>
<li>学习率过度衰减：随着时间的推移，累积的时间步梯度平方值越来越大，导致学习率逐渐接近零，模型会停止学习。</li>
<li>不适合非稀疏数据：在非稀疏数据的情况下，学习率过快衰减可能导致优化过程早期停滞。</li>
</ol>
<p>AdaGrad是一种有效的自适应学习率算法，然而由于学习率衰减问题，我们会使用改 RMSProp 或 Adam 来替代。</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型和损失函数</span></span><br><span class="line">model = torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adagrad(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据</span></span><br><span class="line">X = torch.randn(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">y = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = model(X)</span><br><span class="line">    loss = criterion(outputs, y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="7-3RMSProp"><a href="#7-3RMSProp" class="headerlink" title="7.3RMSProp"></a>7.3RMSProp</h3><p>虽然 AdaGrad 能够自适应地调整学习率，但随着训练进行，累积梯度平方 $G_t$会不断增大，导致学习率逐渐减小，最终可能变得过小，导致训练停滞。</p>
<p>RMSProp（Root Mean Square Propagation）是一种自适应学习率的优化算法，在时间步中，不是简单地累积所有梯度平方和，而是使用指数加权平均来逐步衰减过时的梯度信息。旨在解决 <strong>AdaGrad</strong> 学习率单调递减的问题。它通过引入 <strong>指数加权平均</strong> 来累积历史梯度的平方，从而动态调整学习率。</p>
<p>公式为：<br>$$<br>s_t&#x3D;β⋅s_{t−1}+(1−β)⋅g_t^2\θ_{t+1}&#x3D;θ_t−\frac{η}{\sqrt{s_t+ϵ}}⋅gt<br>$$<br>其中：</p>
<ul>
<li>$s_t$是当前时刻的指数加权平均梯度平方。</li>
<li>β是衰减因子，通常取 0.9。</li>
<li>η是初始学习率。</li>
<li>ϵ是一个小常数（通常取 $10^{−8}$），用于防止除零。</li>
<li>$g_t$是当前时刻的梯度。</li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li><p>适应性强：RMSProp自适应调整每个参数的学习率，对于梯度变化较大的情况非常有效，使得优化过程更加平稳。</p>
</li>
<li><p>适合非稀疏数据：相比于AdaGrad，RMSProp更加适合处理非稀疏数据，因为它不会让学习率减小到几乎为零。</p>
</li>
<li><p>解决过度衰减问题：通过引入指数加权平均，RMSProp避免了AdaGrad中学习率过快衰减的问题，保持了学习率的稳定性</p>
</li>
</ul>
<p><strong>缺点</strong></p>
<p>依赖于超参数的选择：RMSProp的效果对衰减率 $$ \gamma$$ 和学习率 $$ \eta$$ 的选择比较敏感，需要一些调参工作。</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型和损失函数</span></span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.RMSprop(model.parameters(), lr=<span class="number">0.01</span>, alpha=<span class="number">0.9</span>, eps=<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据</span></span><br><span class="line">X = torch.randn(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">y = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = model(X)</span><br><span class="line">    loss = criterion(outputs, y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="7-4Adam"><a href="#7-4Adam" class="headerlink" title="7.4Adam"></a>7.4Adam</h3><p>Adam（Adaptive Moment Estimation）算法将动量法和RMSProp的优点结合在一起：</p>
<ul>
<li><strong>动量法</strong>：通过一阶动量（即梯度的指数加权平均）来加速收敛，尤其是在有噪声或梯度稀疏的情况下。</li>
<li><strong>RMSProp</strong>：通过二阶动量（即梯度平方的指数加权平均）来调整学习率，使得每个参数的学习率适应其梯度的变化。</li>
</ul>
<p><strong>Adam过程</strong></p>
<ol>
<li><p><strong>初始化</strong>：</p>
<ul>
<li>初始化参数 $$ \theta_0$$ 和学习率$$ \eta$$。</li>
<li>初始化一阶动量估计 $$m_0 &#x3D; 0$$ 和二阶动量估计$$ v_0 &#x3D; 0$$。</li>
<li>设定动量项的衰减率 $$ \beta_1$$ 和二阶动量项的衰减率$$ \beta_2$$，通常 $$ \beta_1 &#x3D; 0.9$$，$$ \beta_2 &#x3D; 0.999$$。</li>
<li>设定一个小常数$$ \epsilon$$（通常取$$ 10^{-8}$$），用于防止除零错误。</li>
</ul>
</li>
<li><p><strong>梯度计算</strong>：</p>
<ul>
<li>在每个时间步 $$t$$，计算损失函数$$ J(\theta)$$ 对参数$$ \theta$$ 的梯度$$ g_t &#x3D; \nabla_\theta J(\theta_t) $$。</li>
</ul>
</li>
<li><p><strong>一阶动量估计（梯度的指数加权平均）</strong>：</p>
<ul>
<li>更新一阶动量估计：<br>$$<br>m_t &#x3D; \beta_1 m_{t-1} + (1 - \beta_1) g_t<br>$$<br>其中，$$ m_t$$ 是当前时间步 $$t$$ 的一阶动量估计，表示梯度的指数加权平均。</li>
</ul>
</li>
<li><p><strong>二阶动量估计（梯度平方的指数加权平均）</strong>：</p>
<ul>
<li>更新二阶动量估计：<br>$$<br>v_t &#x3D; \beta_2 v_{t-1} + (1 - \beta_2) g_t^2<br>$$<br>其中，$$v_t$$ 是当前时间步 $$t$$ 的二阶动量估计，表示梯度平方的指数加权平均。</li>
</ul>
</li>
<li><p><strong>偏差校正</strong>：</p>
<p>由于一阶动量和二阶动量在初始阶段可能会有偏差，以二阶动量为例：</p>
<p>在计算指数加权平均时，初始化 $v_{0}&#x3D;0$，那么$v_{1}&#x3D;0.999\cdot v_{0}+0.001\cdot g_{1}^2$，得到$v_{1}&#x3D;0.001\cdot g_{1}^2$，显然得到的 $v_{1}$ 会小很多，导致估计的不准确，以此类推：</p>
<p>根据：$v_{2}&#x3D;0.999\cdot v_{1}+0.001\cdot g_{2}^2$，把 $v_{1}$ 带入后，<br>得到：$v_{2}&#x3D;0.999\cdot 0.001\cdot g_{1}^2+0.001\cdot g_{2}^2$，导致 $v_{2}$ 远小于 $g_{1}$ 和 $g_{2}$，所以 $v_{2}$ 并不能很好的估计出前两次训练的梯度。</p>
<p>所以这个估计是有偏差的。</p>
<p>使用以下公式进行偏差校正：<br>$$<br>\hat{m}_t &#x3D; \frac{m_t}{1 - \beta_1^t}<br>\<br>\hat{v}_t &#x3D; \frac{v_t}{1 - \beta_2^t}<br>$$<br>其中，$$ \hat{m}_t$$ 和$$ \hat{v}_t$$ 是校正后的一阶和二阶动量估计。</p>
<p><strong>推导：</strong></p>
<p>假设梯度 ${g_t}$是一个平稳的随机变量，其期望为：<br>$$<br>E(g_t)&#x3D;μ<br>$$<br>根据指数加权平均的递推公式，我们可以递推计算 ${m_t}$的期望。</p>
<p>当t&#x3D;1：<br>$$<br>m_1&#x3D;β⋅m_0+(1−β)⋅g_1<br>$$<br>假设 $m_0&#x3D;0$，所以：<br>$$<br>m_1&#x3D;(1−β)⋅g_1<br>$$<br>取期望：<br>$$<br>E(m1)&#x3D;(1−β)⋅E(g_1)&#x3D;(1−β)⋅μ<br>$$<br>当t&#x3D;2：<br>$$<br>m_2&#x3D;β⋅m_1+(1−β)⋅g_2<br>$$<br>代入 $m_1&#x3D;(1−β)⋅g_1$：<br>$$<br>m_2&#x3D;β⋅(1−β)⋅g_1+(1−β)⋅g_2<br>$$<br>取期望：<br>$$<br>E(m_2)&#x3D;β⋅(1−β)⋅E(g_1)+(1−β)⋅E(g_2)&#x3D;β(1−β)⋅μ+(1−β)⋅μ&#x3D;(1−β^2)⋅μ<br>$$<br>当t&#x3D;3：<br>$$<br>m_3&#x3D;β⋅m_2+(1−β)⋅g_3<br>$$<br>代入 $m_2&#x3D;β(1−β)⋅g_1+(1−β)⋅g_2$：<br>$$<br>m_3&#x3D;β⋅[β(1−β)⋅g_1+(1−β)⋅g_2]+(1−β)⋅g_3<br>$$<br>展开：<br>$$<br>m_3&#x3D;β^2(1−β)⋅g_1+β(1−β)⋅g_2+(1−β)⋅g_3<br>$$<br>取期望：<br>$$<br>E(m_3)&#x3D;β^2(1−β)⋅μ+β(1−β)⋅μ+(1−β)⋅μ&#x3D;(1−β^3)⋅μ<br>$$<br>通过递推可以发现，对于任意时刻 t，$m_t$的期望为：<br>$$<br>E(m_t)&#x3D;(1−β^t)⋅μ<br>$$<br>通过上述公式我们发现$m_t$的期望与梯度期望相差了$(1−β^t)$倍，我们想要找到一个修正后的$\hat{m_t}$，使：$E(\hat{m_t})&#x3D;μ$</p>
<p>所以：<br>$$<br>E(m_t)&#x3D;(1−β^t)⋅μ&#x3D;(1−β^t)⋅E(\hat{m_t})<br>$$<br>进行缩放：<br>$$<br>m_t&#x3D;(1−β^t)⋅\hat{m_t}<br>$$<br>得出：<br>$$<br>\hat{m_t}&#x3D;\frac{m_t}{1−β^t}<br>$$<br>$v_t$推导同理。</p>
</li>
<li><p><strong>参数更新</strong>：</p>
<ul>
<li>使用校正后的动量估计更新参数：<br>$$<br>\theta_{t+1} &#x3D; \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t<br>$$</li>
</ul>
</li>
</ol>
<p><strong>优点</strong></p>
<ol>
<li><p>高效稳健：Adam结合了动量法和RMSProp的优势，在处理非静态、稀疏梯度和噪声数据时表现出色，能够快速稳定地收敛。</p>
</li>
<li><p>自适应学习率：Adam通过一阶和二阶动量的估计，自适应调整每个参数的学习率，避免了全局学习率设定不合适的问题。</p>
</li>
<li><p>适用大多数问题：Adam几乎可以在不调整超参数的情况下应用于各种深度学习模型，表现良好。</p>
</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li><p>超参数敏感：尽管Adam通常能很好地工作，但它对初始超参数（如 $$ \beta_1$$、$$ \beta_2$$ 和  $$\eta$$）仍然较为敏感，有时需要仔细调参。</p>
</li>
<li><p>过拟合风险：由于Adam会在初始阶段快速收敛，可能导致模型陷入局部最优甚至过拟合。因此，有时会结合其他优化算法（如SGD）使用。</p>
</li>
</ol>
<h3 id="7-5总结"><a href="#7-5总结" class="headerlink" title="7.5总结"></a>7.5总结</h3><p>梯度下降算法通过不断更新参数来最小化损失函数，是反向传播算法中计算权重调整的基础。在实际应用中，根据数据的规模和计算资源的情况，选择合适的梯度下降方式（批量、随机、小批量）及其变种（如动量法、Adam等）可以显著提高模型训练的效率和效果。</p>
<p>​		Adam是目前最为流行的优化算法之一，因其稳定性和高效性，广泛应用于各种深度学习模型的训练中。Adam结合了动量法和RMSProp的优点，能够在不同情况下自适应调整学习率，并提供快速且稳定的收敛表现。</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型和损失函数</span></span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据</span></span><br><span class="line">X = torch.randn(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">y = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = model(X)</span><br><span class="line">    loss = criterion(outputs, y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="8过拟合与欠拟合"><a href="#8过拟合与欠拟合" class="headerlink" title="8过拟合与欠拟合"></a>8过拟合与欠拟合</h2><h3 id="8-1-L1"><a href="#8-1-L1" class="headerlink" title="8.1 L1"></a>8.1 L1</h3><h3 id="8-2-L2"><a href="#8-2-L2" class="headerlink" title="8.2 L2"></a>8.2 L2</h3><p>L1、L2见机器学习学习笔记</p>
<h3 id="8-3dropout"><a href="#8-3dropout" class="headerlink" title="8.3dropout"></a>8.3dropout</h3><h4 id="8-3-1Dropout过程"><a href="#8-3-1Dropout过程" class="headerlink" title="8.3.1Dropout过程"></a>8.3.1Dropout过程</h4><ol>
<li>按照指定的概率把部分神经元的值设置为0；</li>
<li>为了规避该操作带来的影响，需对非 0 的元素使用缩放因子$$1&#x2F;(1-p)$$进行强化。</li>
</ol>
<p>假设某个神经元的输出为 x，Dropout 的操作可以表示为：</p>
<ul>
<li><p>在训练阶段：<br>$$<br>y&#x3D;\begin{cases}\frac{x}{1−p} &amp; 以概率1−p保留神经元 \<br>0 &amp; 以概率 p 丢弃神经元 \end{cases}<br>$$</p>
</li>
<li><p>在测试阶段：<br>$$<br>y&#x3D;x<br>$$</p>
</li>
</ul>
<p>为什么要使用缩放因子$$1&#x2F;(1-p)$$？</p>
<p>在训练阶段，Dropout 会以概率 p随机将某些神经元的输出设置为 0，而以概率 1−p 保留这些神经元。</p>
<p>假设某个神经元的原始输出是 x，那么在训练阶段，它的期望输出值为：<br>$$<br>E(y_{train})&#x3D;(1−p)⋅(\frac{x}{1−p})+p⋅0&#x3D;x<br>$$<br>通过这种缩放，训练阶段的期望输出值仍然是 x，与没有 Dropout 时一致。</p>
<h4 id="8-3-2实际应用"><a href="#8-3-2实际应用" class="headerlink" title="8.3.2实际应用"></a>8.3.2实际应用</h4><p>示例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from PIL import Image</span><br><span class="line">from torchvision import transforms</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(42)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_img(path, resize=(224, 224)):</span><br><span class="line">    pil_img = Image.open(path).convert(&#x27;RGB&#x27;)</span><br><span class="line">    print(&quot;Original image size:&quot;, pil_img.size)  # 打印原始尺寸</span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.Resize(resize),</span><br><span class="line">        transforms.ToTensor()  # 转换为Tensor并自动归一化到[0,1]</span><br><span class="line">    ])</span><br><span class="line">    return transform(pil_img)  # 返回[C,H,W]格式的tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    dirpath = os.path.dirname(__file__)</span><br><span class="line">    path = os.path.join(dirpath, &#x27;img&#x27;, &#x27;100.jpg&#x27;)  # 使用os.path.join更安全</span><br><span class="line"></span><br><span class="line">    # 加载图像 (已经是[0,1]范围的Tensor)</span><br><span class="line">    trans_img = load_img(path)</span><br><span class="line"></span><br><span class="line">    # 添加batch维度 [1, C, H, W]，因为Dropout默认需要4D输入</span><br><span class="line">    trans_img = trans_img.unsqueeze(0)</span><br><span class="line"></span><br><span class="line">    # 创建Dropout层</span><br><span class="line">    dropout = nn.Dropout2d(p=0.2)</span><br><span class="line"></span><br><span class="line">    drop_img = dropout(trans_img)</span><br><span class="line"></span><br><span class="line">    # 移除batch维度并转换为[H,W,C]格式供matplotlib显示</span><br><span class="line">    trans_img = trans_img.squeeze(0).permute(1, 2, 0).numpy()</span><br><span class="line">    drop_img = drop_img.squeeze(0).permute(1, 2, 0).numpy()</span><br><span class="line"></span><br><span class="line">    # 确保数据在[0,1]范围内</span><br><span class="line">    drop_img = drop_img.clip(0, 1)</span><br><span class="line"></span><br><span class="line">    # 显示图像</span><br><span class="line">    fig = plt.figure(figsize=(10, 5))</span><br><span class="line"></span><br><span class="line">    ax1 = fig.add_subplot(1, 2, 1)</span><br><span class="line">    ax1.imshow(trans_img)</span><br><span class="line"></span><br><span class="line">    ax2 = fig.add_subplot(1, 2, 2)</span><br><span class="line">    ax2.imshow(drop_img)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<h2 id="9-数据增强"><a href="#9-数据增强" class="headerlink" title="9.数据增强"></a>9.数据增强</h2><h3 id="9-1优点"><a href="#9-1优点" class="headerlink" title="9.1优点"></a>9.1优点</h3><p>大幅度降低数据采集和标注成本；</p>
<p>​		模型过拟合风险降低，提高模型泛化能力；</p>
<p>官方地址：</p>
<p>transforms：<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/transforms.html">https://pytorch.org/vision/stable/transforms.html</a></p>
<h3 id="9-2常用数据增强变换类"><a href="#9-2常用数据增强变换类" class="headerlink" title="9.2常用数据增强变换类"></a>9.2常用数据增强变换类</h3><ul>
<li>transforms.Compose：将多个变换操作组合成一个流水线。</li>
<li>transforms.ToTensor：将 PIL 图像或 NumPy 数组转换为 PyTorch 张量，将图像数据从 uint8 类型 (0-255) 转换为 float32 类型 (0.0-1.0)。</li>
<li>transforms.Normalize：对张量进行标准化。</li>
<li>transforms.Resize：调整图像大小。</li>
<li>transforms.CenterCrop：从图像中心裁剪指定大小的区域。</li>
<li>transforms.RandomCrop：随机裁剪图像。</li>
<li>transforms.RandomHorizontalFlip：随机水平翻转图像。</li>
<li>transforms.RandomVerticalFlip：随机垂直翻转图像。</li>
<li>transforms.RandomRotation：随机旋转图像。</li>
<li>transforms.ColorJitter：随机调整图像的亮度、对比度、饱和度和色调。</li>
<li>transforms.RandomGrayscale：随机将图像转换为灰度图像。</li>
<li>transforms.RandomResizedCrop：随机裁剪图像并调整大小。</li>
</ul>
<h2 id="10批量标准化"><a href="#10批量标准化" class="headerlink" title="10批量标准化"></a>10批量标准化</h2><h3 id="10-1优点"><a href="#10-1优点" class="headerlink" title="10.1优点"></a>10.1优点</h3><p>批量标准化（Batch Normalization, BN）是一种广泛使用的神经网络正则化技术，核心思想是对每一层的输入进行标准化，然后进行缩放和平移，旨在加速训练、提高模型的稳定性和泛化能力。批量标准化通常在<strong>全连接层</strong>或<strong>卷积层</strong>之后、激活函数之前应用。</p>
<p><strong>核心思想</strong></p>
<p>Batch Normalization（BN）通过对每一批（batch）数据的每个特征通道进行标准化，解决<strong>内部协变量偏移</strong>（Internal Covariate Shift）问题，从而：</p>
<ul>
<li>加速网络训练</li>
<li>允许使用更大的学习率</li>
<li>减少对初始化的依赖</li>
<li>提供轻微的正则化效果</li>
</ul>
<p>自动更新内部的参数：缩放因子 $$\gamma$$ 和偏移量 $$\beta$$。</p>
<h3 id="10-2示例"><a href="#10-2示例" class="headerlink" title="10.2示例"></a>10.2示例</h3><p><code>torch.nn.BatchNorm1d</code> 是 PyTorch 中用于一维数据的批量标准化（Batch Normalization）模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BatchNorm1d(</span><br><span class="line">    num_features,         <span class="comment"># 输入数据的特征维度</span></span><br><span class="line">    eps=<span class="number">1e-05</span>,           <span class="comment"># 用于数值稳定性的小常数</span></span><br><span class="line">    momentum=<span class="number">0.1</span>,        <span class="comment"># 用于计算全局统计量的动量</span></span><br><span class="line">    affine=<span class="literal">True</span>,         <span class="comment"># 是否启用可学习的缩放和平移参数</span></span><br><span class="line">    track_running_stats=<span class="literal">True</span>,  <span class="comment"># 是否跟踪全局统计量</span></span><br><span class="line">    device=<span class="literal">None</span>,         <span class="comment"># 设备类型（如 CPU 或 GPU）</span></span><br><span class="line">    dtype=<span class="literal">None</span>           <span class="comment"># 数据类型</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<p>eps：用于数值稳定性的小常数，添加到方差的分母中，防止除零错误。默认值：1e-05</p>
<p>momentum：用于计算全局统计量（均值和方差）的动量。默认值：0.1，参考本节1.4</p>
<p>affine：是否启用可学习的缩放和平移参数（γ和 β）。如果 affine&#x3D;True，则模块会学习两个参数；如果 affine&#x3D;False，则不学习参数，直接输出标准化后的值 $\hat x_i$。默认值：True</p>
<p>track_running_stats：是否跟踪全局统计量（均值和方差）。如果 track_running_stats&#x3D;True，则在训练过程中计算并更新全局统计量，并在测试阶段使用这些统计量。如果 track_running_stats&#x3D;False，则不跟踪全局统计量，每次标准化都使用当前 mini-batch 的统计量。默认值：True</p>
<h2 id="11其他"><a href="#11其他" class="headerlink" title="11其他"></a>11其他</h2><h3 id="11-1模型搭建"><a href="#11-1模型搭建" class="headerlink" title="11.1模型搭建"></a>11.1模型搭建</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class MyModle(nn.Module):</span><br><span class="line">    def __init__(self, input_size, output_size):</span><br><span class="line">        super(MyModle, self).__init__()</span><br><span class="line">        # 创建一个全连接网络(full connected layer)</span><br><span class="line">        self.fc1 = nn.Linear(input_size, 128)</span><br><span class="line">        self.fc2 = nn.Linear(128, 64)</span><br><span class="line">        self.fc3 = nn.Linear(64, output_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        output = self.fc3(x)</span><br><span class="line">        return output</span><br><span class="line">    </span><br><span class="line"># 创建模型实例</span><br><span class="line">model = MyModel(input_size=10, output_size=2)</span><br><span class="line"># 输入数据</span><br><span class="line">x = torch.randn(5, 10)</span><br><span class="line"># 调用模型</span><br><span class="line">output = model(x)</span><br></pre></td></tr></table></figure>

<p>继承nn.Module</p>
<p>forward 方法是 PyTorch 中 nn.Module 类的<strong>必须实现的方法</strong>。它是定义神经网络前向传播逻辑的地方，决定了数据如何通过网络层传递并生成输出。同时forward 方法定义了计算图，PyTorch 会根据这个计算图自动计算梯度并更新参数。</p>
<h3 id="11-2模型的保存与载入"><a href="#11-2模型的保存与载入" class="headerlink" title="11.2模型的保存与载入"></a>11.2模型的保存与载入</h3><h4 id="11-2-1模型保存"><a href="#11-2-1模型保存" class="headerlink" title="11.2.1模型保存"></a>11.2.1模型保存</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#保存模型</span><br><span class="line">def test01():</span><br><span class="line">    model=MyNet()</span><br><span class="line">    print(model)</span><br><span class="line">    torch.save(model,&#x27;./model/model.pt&#x27;)</span><br></pre></td></tr></table></figure>

<h4 id="11-2-2模型载入"><a href="#11-2-2模型载入" class="headerlink" title="11.2.2模型载入"></a>11.2.2模型载入</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#加载模型  </span><br><span class="line">#是完整的模型对象，可以直接使用</span><br><span class="line">def test02():</span><br><span class="line">    model=torch.load(&#x27;./model/model.pt&#x27;)</span><br><span class="line">    print(model)</span><br></pre></td></tr></table></figure>

<h4 id="11-2-3模型参数保存"><a href="#11-2-3模型参数保存" class="headerlink" title="11.2.3模型参数保存"></a>11.2.3模型参数保存</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#保存模型参数   </span><br><span class="line">def test03():</span><br><span class="line">    model=MyNet()</span><br><span class="line">    state_dict=&#123;</span><br><span class="line">        &quot;init_params&quot;: &#123;</span><br><span class="line">            &quot;input_size&quot;: 128,  # 输入特征数</span><br><span class="line">            &quot;output_size&quot;: 32,  # 输出特征数</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;accuracy&quot;: 0.99,  # 模型准确率</span><br><span class="line">        &quot;model_state_dict&quot;: model.state_dict(),</span><br><span class="line">        &quot;optimizer_state_dict&quot;: optimizer.state_dict(),</span><br><span class="line">    &#125;</span><br><span class="line">    torch.save(state_dict,&#x27;./model/model_dict.pt&#x27;)</span><br></pre></td></tr></table></figure>

<h4 id="11-2-4模型参数载入"><a href="#11-2-4模型参数载入" class="headerlink" title="11.2.4模型参数载入"></a>11.2.4模型参数载入</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#加载模型参数</span><br><span class="line">#如果保存的是模型参数，加载的模型数据是字典，内容是模型参数，并不是完整的模型</span><br><span class="line">#需要实现初始化模型，然后把模型参数带入</span><br><span class="line">def test04():</span><br><span class="line">    model=MyNet()</span><br><span class="line">    state_dict=torch.load(&#x27;./model/model_dict.pt&#x27;)</span><br><span class="line">    model.load_state_dict(state_dict)</span><br></pre></td></tr></table></figure>

<h2 id="12全连接网络项目实战"><a href="#12全连接网络项目实战" class="headerlink" title="12全连接网络项目实战"></a>12全连接网络项目实战</h2><h3 id="12-1导入包"><a href="#12-1导入包" class="headerlink" title="12.1导入包"></a>12.1导入包</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.utils.data import DataLoader,Dataset</span><br><span class="line">from torchvision import datasets,transforms</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br></pre></td></tr></table></figure>

<h3 id="12-2搭建网络"><a href="#12-2搭建网络" class="headerlink" title="12.2搭建网络"></a>12.2搭建网络</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class MyNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(32 * 32 * 3, 1024)</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(1024)</span><br><span class="line">        self.dropout1 = nn.Dropout(0.4)</span><br><span class="line">        self.fc2 = nn.Linear(1024, 512)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(512)</span><br><span class="line">        self.dropout2 = nn.Dropout(0.3)</span><br><span class="line">        self.fc3 = nn.Linear(512, 256)  # 增加第三层</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(256)</span><br><span class="line">        self.fc4 = nn.Linear(256, 10)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = x.view(-1, 32 * 32 * 3)</span><br><span class="line">        x = self.dropout1(self.bn1(self.fc1(x)))</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.dropout2(self.bn2(self.fc2(x)))</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.bn3(self.fc3(x))</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.fc4(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<h3 id="12-3构造训练函数"><a href="#12-3构造训练函数" class="headerlink" title="12.3构造训练函数"></a>12.3构造训练函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def train(model,train_loader,epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        correct=0</span><br><span class="line">        for data,target in train_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            pre_y=model(data)</span><br><span class="line">            loss=criterion(pre_y,target)</span><br><span class="line">            opt.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            </span><br><span class="line">            _,predicted=torch.max(pre_y.data,1)#获取最大概率的索引</span><br><span class="line">            </span><br><span class="line">            correct+=predicted.eq(target.data).sum().item()#计算准确率</span><br><span class="line">        correct=correct/len(train_loader.dataset)#计算准确率</span><br><span class="line">        print(&#x27;Epoch:&#123;&#125;  Loss:&#123;:.1f&#125;  Accuracy:&#123;:.1f&#125;&#x27;.format(epoch+1,loss.item(),correct))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="12-4构造验证函数"><a href="#12-4构造验证函数" class="headerlink" title="12.4构造验证函数"></a>12.4构造验证函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def eval(model,eval_loader):</span><br><span class="line">    model.eval()</span><br><span class="line">    eval_loss=0</span><br><span class="line">    correct=0</span><br><span class="line">    </span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for data,target in eval_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            pre_y=model(data)</span><br><span class="line">            eval_loss+=criterion(pre_y,target).item()</span><br><span class="line">            _,predicted=torch.max(pre_y.data,1)</span><br><span class="line">            correct+=predicted.eq(target.data).sum().item()</span><br><span class="line">        eval_loss/=len(eval_loader.dataset)</span><br><span class="line">        acc=100.0*correct/len(eval_loader.dataset)</span><br><span class="line">        print(&#x27;Test Loss:&#123;:.6f&#125;  Accuracy:&#123;&#125;%&#x27;.format(eval_loss,round(acc)))</span><br><span class="line">    return acc</span><br></pre></td></tr></table></figure>

<h3 id="12-5构建主函数"><a href="#12-5构建主函数" class="headerlink" title="12.5构建主函数"></a>12.5构建主函数</h3><h4 id="12-5-1构建数据增强转换器"><a href="#12-5-1构建数据增强转换器" class="headerlink" title="12.5.1构建数据增强转换器"></a>12.5.1构建数据增强转换器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">transform=transforms.Compose([</span><br><span class="line">        # transforms.RandomCrop(32,padding=4),</span><br><span class="line">        # transforms.RandomHorizontalFlip(),</span><br><span class="line">        # transforms.RandomRotation((30.90)),</span><br><span class="line">        # transforms.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5,hue=0.5),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>



<h4 id="12-5-2构建数据加载器"><a href="#12-5-2构建数据加载器" class="headerlink" title="12.5.2构建数据加载器"></a>12.5.2构建数据加载器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#加载数据集</span><br><span class="line">   train_dataset= datasets.CIFAR10(root=&#x27;code/cifar10&#x27;,train=True,transform=transform,download=True)</span><br><span class="line">   eval_dataset= datasets.CIFAR10(root=&#x27;code/cifar10&#x27;,train=False,transform=transform,download=True)</span><br><span class="line">       </span><br><span class="line">   train_dataloader=DataLoader(dataset=train_dataset,batch_size=64,shuffle=True)</span><br><span class="line">   eval_dataloader=DataLoader(dataset=eval_dataset,batch_size=64,shuffle=False)</span><br></pre></td></tr></table></figure>

<h4 id="12-5-3载入模型并设为cuda运行"><a href="#12-5-3载入模型并设为cuda运行" class="headerlink" title="12.5.3载入模型并设为cuda运行"></a>12.5.3载入模型并设为cuda运行</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model=MyNet()</span><br><span class="line">device=torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">model.to(device)            # 你的网络</span><br></pre></td></tr></table></figure>

<h3 id="12-6载入数据运行代码"><a href="#12-6载入数据运行代码" class="headerlink" title="12.6载入数据运行代码"></a>12.6载入数据运行代码</h3>
        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>CXZ</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://cxz-deman.github.io/2025/05/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">https://cxz-deman.github.io/2025/05/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>god</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%90%88%E9%9B%86/"># 阶段学习笔记合集</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2025/07/21/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络</a>
            
            
            <a class="next" rel="next" href="/2025/05/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">机器学习</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© CXZ | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>